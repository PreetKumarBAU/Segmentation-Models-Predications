{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.4.1\n!pip install keras==2.4.3","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"PNfpi6Q0jXrM","execution":{"iopub.status.busy":"2021-09-23T19:37:55.425850Z","iopub.execute_input":"2021-09-23T19:37:55.426484Z","iopub.status.idle":"2021-09-23T19:38:11.164720Z","shell.execute_reply.started":"2021-09-23T19:37:55.426355Z","shell.execute_reply":"2021-09-23T19:38:11.163758Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\n#import keras.backend as K\n\n#%tensorflow_version 1.x\nimport os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T19:38:11.168354Z","iopub.execute_input":"2021-09-23T19:38:11.168583Z","iopub.status.idle":"2021-09-23T19:38:15.490710Z","shell.execute_reply.started":"2021-09-23T19:38:11.168555Z","shell.execute_reply":"2021-09-23T19:38:15.489834Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"id":"2a8AfzPOjXrQ","execution":{"iopub.status.busy":"2021-09-23T19:38:15.492162Z","iopub.execute_input":"2021-09-23T19:38:15.492410Z","iopub.status.idle":"2021-09-23T19:38:16.196936Z","shell.execute_reply.started":"2021-09-23T19:38:15.492378Z","shell.execute_reply":"2021-09-23T19:38:16.196236Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=256, img_cols=256,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=20,\n                                  width_shift_range=0.002,\n                                  shear_range=0.03,\n                                  zoom_range=0.005,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (256, 256)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        # return imgs,labels\n\n    def saveResult(self, npyfile, size, name,threshold=80):\n        for i, item in enumerate(npyfile):\n            img = item\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"id":"iTf8lIwLjXrS","execution":{"iopub.status.busy":"2021-09-23T19:38:16.199169Z","iopub.execute_input":"2021-09-23T19:38:16.199439Z","iopub.status.idle":"2021-09-23T19:38:16.683860Z","shell.execute_reply.started":"2021-09-23T19:38:16.199406Z","shell.execute_reply":"2021-09-23T19:38:16.683087Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"####  Metrics\n\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss\ndef ACL5_mod(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out*1.4)) \n\n\treturn loss","metadata":{"id":"VVdzJaSWjXra","execution":{"iopub.status.busy":"2021-09-23T19:38:16.685268Z","iopub.execute_input":"2021-09-23T19:38:16.685534Z","iopub.status.idle":"2021-09-23T19:38:16.711101Z","shell.execute_reply.started":"2021-09-23T19:38:16.685499Z","shell.execute_reply":"2021-09-23T19:38:16.710267Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iwu9rjDFjXrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_coef(y_true, y_pred, smooth=1):\n  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n  return dice\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"id":"-YuW43lejXrd","execution":{"iopub.status.busy":"2021-09-23T19:38:16.714069Z","iopub.execute_input":"2021-09-23T19:38:16.714266Z","iopub.status.idle":"2021-09-23T19:38:16.726343Z","shell.execute_reply.started":"2021-09-23T19:38:16.714245Z","shell.execute_reply":"2021-09-23T19:38:16.725614Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"id":"kr5nP8ItjXre","outputId":"48e77d7c-c144-4f17-e726-e4b36dce7171","execution":{"iopub.status.busy":"2021-09-23T19:38:16.727748Z","iopub.execute_input":"2021-09-23T19:38:16.728339Z","iopub.status.idle":"2021-09-23T19:38:16.742537Z","shell.execute_reply.started":"2021-09-23T19:38:16.728303Z","shell.execute_reply":"2021-09-23T19:38:16.741577Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"id":"KMTFbTKtjXrf","outputId":"2bd1e0fe-2f4f-479d-e8e6-b255673c5ad3","execution":{"iopub.status.busy":"2021-09-23T19:38:16.744003Z","iopub.execute_input":"2021-09-23T19:38:16.744315Z","iopub.status.idle":"2021-09-23T19:38:24.356528Z","shell.execute_reply.started":"2021-09-23T19:38:16.744279Z","shell.execute_reply":"2021-09-23T19:38:24.355732Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n\ndef focal_tversky_bce_jaccard_loss(y_true, y_pred):\n    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n","metadata":{"id":"IVcpwi4hjXrg","execution":{"iopub.status.busy":"2021-09-23T19:38:24.358131Z","iopub.execute_input":"2021-09-23T19:38:24.358394Z","iopub.status.idle":"2021-09-23T19:38:24.363225Z","shell.execute_reply.started":"2021-09-23T19:38:24.358360Z","shell.execute_reply":"2021-09-23T19:38:24.362487Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2","metadata":{"id":"eDJ9CtzQjXrh","execution":{"iopub.status.busy":"2021-09-23T19:38:24.366573Z","iopub.execute_input":"2021-09-23T19:38:24.367009Z","iopub.status.idle":"2021-09-23T19:38:24.373971Z","shell.execute_reply.started":"2021-09-23T19:38:24.366969Z","shell.execute_reply":"2021-09-23T19:38:24.373319Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#  DenseNet201_UNet","metadata":{"id":"ccYNxPZwjXri"}},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2\ndef conv_block(inputs , num_filters):\n  x = Conv2D(num_filters , 3 , padding= \"same\" )(inputs)\n  x = BatchNormalization()(x)\n  x = Activation(\"relu\")(x)\n  x = Dropout(0.3)(x)\n\n  x = Conv2D(num_filters , 3 , padding= \"same\" )(x)\n  x = BatchNormalization()(x)\n  x = Activation(\"relu\")(x)\n  x = Dropout(0.3)(x)\n\n  return x\n\n\ndef decoder_block(inputs , skip_features , num_filters):\n\n  x = Conv2DTranspose(num_filters , (2, 2) , strides = 2 , padding = \"same\")(inputs)\n  x = Concatenate()([x , skip_features ])\n  x = conv_block(x , num_filters)\n\n  return x\n\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import ResNet50\n\n\ndef build_DenseNet210_Unet(input_shape):\n\n  ##Input\n  inputs = Input(input_shape)\n\n  ## Pre-Trained Encoder\n\n  encoder = DenseNet201(include_top = False , weights = \"imagenet\" , input_tensor = inputs)\n\n  # Skip Connections which are the Features and we will access these Features and Feed it in the Decoder\n  #First Feature is the input shaped image\n  s1 = encoder.get_layer(\"input_1\").output                         # 256 * 256\n\n  s2 = encoder.get_layer(\"conv1/relu\").output                       # 128 * 128 \n\n  s3 = encoder.get_layer(\"pool2_relu\").output       # 64 * 64 \n\n  s4 = encoder.get_layer(\"pool3_relu\").output       # 32 * 32\n\n  ## BottleNeck or bridge \n  b1 = encoder.get_layer(\"pool4_relu\").output       # 16 * 16\n\n  ## Decoder\n  d1 = decoder_block(b1 , s4 , 512)                                # 32 * 32 * 512 where 512 is the number of Features we extracted \n\n  d2 = decoder_block(d1 , s3 , 256)                                # 64 * 64\n\n  d3 = decoder_block(d2 , s2 , 128)                                # 128 * 128\n\n  d4 = decoder_block(d3 , s1 , 64)                                # 128 * 128\n\n  ## OUTPUT \n  x = Conv2D( 2 ,  1  ,  padding = \"same\" ,  activation = \"relu\")(d4)\n  outputs = Conv2D( 1 ,  1  ,  padding = \"same\" ,  activation = \"sigmoid\")(x)\n\n  ## MODEL \n\n  model = Model(inputs , outputs ,  name = \"DenseNet210\")\n\n  return model\n  #encoder.summary()","metadata":{"id":"SU-bbp33jXrj","execution":{"iopub.status.busy":"2021-09-23T19:38:24.376580Z","iopub.execute_input":"2021-09-23T19:38:24.376768Z","iopub.status.idle":"2021-09-23T19:38:24.391350Z","shell.execute_reply.started":"2021-09-23T19:38:24.376747Z","shell.execute_reply":"2021-09-23T19:38:24.390586Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lrate = 7.00E-05 \n\n    \nif __name__ == \"__main__\":\n    input_shape = (256, 256, 3)\n    model = build_DenseNet210_Unet(input_shape)\n    model.summary()\n    model_checkpoint1 = keras.callbacks.ModelCheckpoint('att_r2_unet.hdf5', monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\n    csv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\n    model.compile(optimizer=Adam(lr=lrate), loss=ACL5 , metrics=[ACL5 ,bce_jaccard_loss , dice_coef , dsc,  dice_loss,iou_coeff,precision,recall])","metadata":{"id":"kFchkooujXrk","outputId":"cc29d1b8-4e58-475a-baab-26468e90c067","execution":{"iopub.status.busy":"2021-09-23T19:38:24.392502Z","iopub.execute_input":"2021-09-23T19:38:24.392774Z","iopub.status.idle":"2021-09-23T19:38:31.800076Z","shell.execute_reply.started":"2021-09-23T19:38:24.392741Z","shell.execute_reply":"2021-09-23T19:38:31.799383Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"   \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\n#path to images\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=1)\ntest_data = dp.testGenerator()\nlrate = 7.00E-05 \n\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D","metadata":{"id":"z73b0ddqjXrl","outputId":"a89c753e-139b-4b08-a496-19df793d5048","execution":{"iopub.status.busy":"2021-09-23T19:38:31.801058Z","iopub.execute_input":"2021-09-23T19:38:31.801321Z","iopub.status.idle":"2021-09-23T19:38:31.809802Z","shell.execute_reply.started":"2021-09-23T19:38:31.801287Z","shell.execute_reply":"2021-09-23T19:38:31.809091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#history = model.fit_generator(train_data,\n#                              steps_per_epoch=1912,epochs=40,\n#                              validation_steps=207,\n#                              validation_data=valid_data,\n#                              callbacks=[model_checkpoint1,csv_logger])","metadata":{"id":"DJLOnaYejXrm","outputId":"deabad6b-2cf2-449a-8124-ea3bd1e2a072","execution":{"iopub.status.busy":"2021-09-23T19:38:31.811143Z","iopub.execute_input":"2021-09-23T19:38:31.811601Z","iopub.status.idle":"2021-09-23T19:38:31.823747Z","shell.execute_reply.started":"2021-09-23T19:38:31.811560Z","shell.execute_reply":"2021-09-23T19:38:31.822962Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"../input/dense210unet-weights/DenseUnet210_ACL.hdf5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T19:38:31.825011Z","iopub.execute_input":"2021-09-23T19:38:31.825329Z","iopub.status.idle":"2021-09-23T19:38:34.596448Z","shell.execute_reply.started":"2021-09-23T19:38:31.825295Z","shell.execute_reply":"2021-09-23T19:38:34.595489Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# DenseNet210 Predications on Test Data ","metadata":{"id":"nR7LOc44jXro"}},{"cell_type":"code","source":"!pip install patchify\nfrom keras.utils import normalize\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify, unpatchify\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n\n    return x\n\ntest_path = '../input/imagestest/Images_to_Test/2'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + test_image_folder\ntest_label_folder =\"labels\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n    \n    x = trans.resize(x, (256, 256), mode='constant')\n\n    return x\n\ndef read_mask(path):\n    #x = io.imread(path, as_gray=False)\n    #x = x / 255.0\n    #x = trans.resize(x, (256, 256), mode='constant')\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, ( 256 , 256))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\n\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([256 , 256 , 3])\n  y.set_shape([256, 256, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/2/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/2/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n  \ntest_path = '../input/imagestest/Images_to_Test/1'\nfrom tqdm import tqdm\n  \n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/1/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/1/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n\nos.chdir(r'/kaggle/working')\n\nfrom zipfile import ZipFile\nimport os\nfrom os.path import basename\ndirName = \"./RESULT\"\n# create a ZipFile object\nwith ZipFile('sampleDir.zip', 'w') as zipObj:\n   # Iterate over all the files in directory\n   for folderName, subfolders, filenames in os.walk(dirName):\n       for filename in filenames:\n           #create complete filepath of file in directory\n           filePath = os.path.join(folderName, filename)\n           # Add file to zip\n           zipObj.write(filePath, basename(filePath))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T19:38:34.598404Z","iopub.execute_input":"2021-09-23T19:38:34.598636Z","iopub.status.idle":"2021-09-23T19:38:53.688267Z","shell.execute_reply.started":"2021-09-23T19:38:34.598596Z","shell.execute_reply":"2021-09-23T19:38:53.687096Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_Test\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/testdata/Test/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/testdata/Test/label'+ \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    \n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        if read_image(x).shape[-1] == 4:\n\n          x = read_image(x)[:,:,:-1]\n        else:\n          x = read_image(x)\n\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_Test/{name}.png\", image)\n\niou_score_list = []\n\nprecision_score_list = []\nrecall_score_list = []\ndice_coeff_score_list = []\n\nfor i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n    if read_image(x).shape[-1] == 4:\n\n      x = read_image(x)[:,:,:-1]\n    else:\n      x = read_image(x)\n    y = read_mask(y)\n    \n    #y_pred = model.predict(np.expand_dims(x, axis=0))\n    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n    y_pred = np.reshape(y_pred , -1)\n    y_pred = np.array([ 1.0 if values else 0.0 for values in y_pred])\n    y = y > 0.5\n    y = np.reshape(y , -1)\n    y = np.array([ 1.0 if values else 0.0 for values in y])\n    y_pred = np.reshape(y_pred , (256 ,256 , 1))\n    y = np.reshape(y , (256 ,256 , 1))\n    #y = y.astype(\"float32\")\n    #y_pred = y_pred.astype(\"float32\")\n    \n    \n    iou_score_list.append(iou_coeff(y , y_pred))\n    precision_score_list.append(precision(y , y_pred))\n    recall_score_list.append(recall(y , y_pred))\n    dice_coeff_score_list.append(dice_coeff(y , y_pred))\n\nprint(\"Average_IOU :\", np.mean(iou_score_list)) \nprint(\"Average_Precision :\", np.mean(precision_score_list)) \nprint(\"Average_Recall :\", np.mean(recall_score_list)) \nprint(\"Average_Dice_Coeff :\", np.mean(dice_coeff_score_list)) \n\nimport shutil\n\nshutil.make_archive(\"RESULT_Test\", 'zip', \"./RESULT_Test\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T19:38:53.689957Z","iopub.execute_input":"2021-09-23T19:38:53.690407Z","iopub.status.idle":"2021-09-23T19:40:39.208614Z","shell.execute_reply.started":"2021-09-23T19:38:53.690367Z","shell.execute_reply":"2021-09-23T19:40:39.207905Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\n\nFileLink(r'./RESULT_Test.zip')","metadata":{"id":"y84NzOdzjXrp","execution":{"iopub.status.busy":"2021-09-23T19:40:39.209966Z","iopub.execute_input":"2021-09-23T19:40:39.210330Z","iopub.status.idle":"2021-09-23T19:40:39.216551Z","shell.execute_reply.started":"2021-09-23T19:40:39.210292Z","shell.execute_reply":"2021-09-23T19:40:39.215811Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"patch_size=256\nfrom PIL import Image\nlarge_image = io.imread('../input/imagestest/Images_to_Test/1/images/i1_.png' , as_gray = False )\nlarge_image = large_image/ 255.0\nlarge_image = trans.resize(large_image, (2560, 2560), mode='constant')\n\npatches = patchify(large_image, ( 256 , 256 , 3), step= 256 )  #Step=256 for 256 patches means no overlap\n\npredicted_patches = []\nfor i in range(patches.shape[0]):\n    for j in range(patches.shape[1]):\n        #print(i,j)\n        \n        single_patch = patches[i,j,:,: ,:]   # (1 , 256 , 256 , 3)\n        \n        \n        #single_patch = single_patch/255.0\n        #single_patch_norm = np.expand_dims(np.array(single_patch), axis=1)\n        single_patch=np.expand_dims(single_patch, 0)\n        #print(single_patch.shape)\n        \n#Predict and threshold for values above 0.5 probability\n        single_patch_prediction = (model.predict(single_patch[0,:,:,:])[0] > 0.5)\n        single_patch_prediction = single_patch_prediction * 255.0\n        predicted_patches.append(single_patch_prediction)\n\npredicted_patches = np.array(predicted_patches)\n\n\npredicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 256,256) )\nreconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape[:2])\nplt.imshow(reconstructed_image, cmap='gray')\nplt.imsave('./segm1.png', reconstructed_image, cmap='gray')\n\nplt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n\nfinal_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(221)\nplt.title('Large Image')\nplt.imshow(large_image, cmap='gray')\nplt.subplot(222)\nplt.title('Prediction of large Image')\nplt.imshow(reconstructed_image, cmap='gray')\nplt.subplot(223)\nplt.title('final_prediction')\nplt.imshow(final_prediction, cmap='gray')\nplt.show()","metadata":{"id":"9zuUHlYwjXrs","execution":{"iopub.status.busy":"2021-09-23T19:40:39.217822Z","iopub.execute_input":"2021-09-23T19:40:39.218499Z","iopub.status.idle":"2021-09-23T19:40:49.933642Z","shell.execute_reply.started":"2021-09-23T19:40:39.218461Z","shell.execute_reply":"2021-09-23T19:40:49.932931Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"JcROpUrFjXrs","outputId":"e2364a58-083a-4e93-8e00-6690beee2c1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"bYd43FdDjXrt","outputId":"2771ab44-fe22-40fb-baff-4b674f4b1312","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"hjfMb9BnjXrt","outputId":"69b96855-6e0e-42f1-f143-e37f10197229","trusted":true},"execution_count":null,"outputs":[]}]}