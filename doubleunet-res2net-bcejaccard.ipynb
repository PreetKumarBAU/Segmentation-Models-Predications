{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.4.1\n!pip install keras==2.4.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-23T17:40:58.940287Z","iopub.execute_input":"2021-09-23T17:40:58.941035Z","iopub.status.idle":"2021-09-23T17:41:14.521977Z","shell.execute_reply.started":"2021-09-23T17:40:58.940921Z","shell.execute_reply":"2021-09-23T17:41:14.521160Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\n#import keras.backend as K\n\n#%tensorflow_version 1.x\nimport os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:14.525787Z","iopub.execute_input":"2021-09-23T17:41:14.525997Z","iopub.status.idle":"2021-09-23T17:41:18.882105Z","shell.execute_reply.started":"2021-09-23T17:41:14.525971Z","shell.execute_reply":"2021-09-23T17:41:18.881236Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:18.883616Z","iopub.execute_input":"2021-09-23T17:41:18.883854Z","iopub.status.idle":"2021-09-23T17:41:19.598634Z","shell.execute_reply.started":"2021-09-23T17:41:18.883822Z","shell.execute_reply":"2021-09-23T17:41:19.597708Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=256, img_cols=256,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=20,\n                                  width_shift_range=0.002,\n                                  shear_range=0.03,\n                                  zoom_range=0.005,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (256, 256)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        # return imgs,labels\n\n    def saveResult(self, npyfile, size, name,threshold=80):\n        for i, item in enumerate(npyfile):\n            img = item\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:19.601373Z","iopub.execute_input":"2021-09-23T17:41:19.601643Z","iopub.status.idle":"2021-09-23T17:41:20.215782Z","shell.execute_reply.started":"2021-09-23T17:41:19.601610Z","shell.execute_reply":"2021-09-23T17:41:20.215087Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"####  Metrics\n\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss\ndef ACL5_mod(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out*1.4)) \n\n\treturn loss","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:20.217169Z","iopub.execute_input":"2021-09-23T17:41:20.217424Z","iopub.status.idle":"2021-09-23T17:41:20.240484Z","shell.execute_reply.started":"2021-09-23T17:41:20.217376Z","shell.execute_reply":"2021-09-23T17:41:20.239358Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:20.241689Z","iopub.execute_input":"2021-09-23T17:41:20.242100Z","iopub.status.idle":"2021-09-23T17:41:20.254659Z","shell.execute_reply.started":"2021-09-23T17:41:20.242069Z","shell.execute_reply":"2021-09-23T17:41:20.253853Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:20.257817Z","iopub.execute_input":"2021-09-23T17:41:20.258156Z","iopub.status.idle":"2021-09-23T17:41:20.267012Z","shell.execute_reply.started":"2021-09-23T17:41:20.258131Z","shell.execute_reply":"2021-09-23T17:41:20.266005Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:20.268293Z","iopub.execute_input":"2021-09-23T17:41:20.268751Z","iopub.status.idle":"2021-09-23T17:41:28.544623Z","shell.execute_reply.started":"2021-09-23T17:41:20.268717Z","shell.execute_reply":"2021-09-23T17:41:28.543880Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n\ndef focal_tversky_bce_jaccard_loss(y_true, y_pred):\n    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:28.546365Z","iopub.execute_input":"2021-09-23T17:41:28.546670Z","iopub.status.idle":"2021-09-23T17:41:28.551780Z","shell.execute_reply.started":"2021-09-23T17:41:28.546630Z","shell.execute_reply":"2021-09-23T17:41:28.551093Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:28.554646Z","iopub.execute_input":"2021-09-23T17:41:28.555135Z","iopub.status.idle":"2021-09-23T17:41:28.561445Z","shell.execute_reply.started":"2021-09-23T17:41:28.555100Z","shell.execute_reply":"2021-09-23T17:41:28.560744Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add,average,concatenate , Conv2DTranspose\nfrom keras.layers.core import Lambda\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\n#import tensorflow as tf\n#import keras.backend as K\n\nfrom keras.layers import Lambda, SeparableConv2D\n\nIMG_SIZE = 256\nh_heuns_method=0.5\n\ndef res_block(x, nb_filters, strides):\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(x)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    \n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n    shortcut = Activation(activation='relu')(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n    return res_path\ndef res_block2(x,y,nb_filters, strides):\n    \n    \n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(x)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    \n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n    shortcut = Activation(activation='relu')(shortcut)\n    \n    res_path = add([shortcut, hpath])#suma corta\n    res_path = BatchNormalization()(res_path)  \n    res_path = Activation(activation='relu')(res_path)\n    # I think it will come , as we want to make y and res_path having SIMILIAR VALUES\n    \n    res_path = average([y, res_path])#suma doble \n    return res_path\n\n\ndef encoder(x):\n    to_decoder = []\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1 , 1))(x)\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1) )(main_path)\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(main_path)\n\n    shortcut = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1)  )(x)\n    shortcut = BatchNormalization()(shortcut)\n    main_path = Activation(activation='relu')(main_path)\n    main_path = add([shortcut, hpath])#suma corta\n\n    to_decoder.append(main_path)\n\n\n    s1 = Conv2D(filters=128, kernel_size=(1, 1), strides=(2, 2) )(x)\n    s1 = BatchNormalization()(s1)\n    s1 = Activation(activation='relu')(s1)\n    \n    main_path = res_block2(main_path,s1, [128, 128], [(2, 2), (1, 1)]) \n    to_decoder.append(main_path)\n\n    main_path = res_block(main_path, [256, 256], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    s2 = Conv2D(filters=512, kernel_size=(1, 1), strides=(4, 4) )(to_decoder[1])\n    s2 = BatchNormalization()(s2)\n    s2 = Activation(activation='relu')(s2)\n    \n    main_path = res_block2(main_path,s2, [512, 512], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    return to_decoder\n\n\ndef decoder(x, from_encoder):\n    \n    main_path = UpSampling2D(size=(2, 2))(x)#32x32\n    #main_path = Conv2DTranspose(512, [2, 2], strides=[2, 2])(x)\n    main_path1 = concatenate([main_path, from_encoder[3]], axis=3)\n    main_path = res_block(main_path1, [512, 512], [(1, 1), (1, 1)])\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)###64x64\n    #main_path = Conv2DTranspose(512, [2, 2], strides=[2, 2])(main_path)\n    main_path = concatenate([main_path, from_encoder[2]], axis=3)#\n    u1 = UpSampling2D(size=(2, 2))(main_path1)#\n    #u1 = Conv2DTranspose(256, [2, 2], strides=[2, 2])(main_path1)\n    u1 = Conv2D(256, kernel_size=(1, 1),strides=(1, 1))(u1)\n    u1 = BatchNormalization()(u1)\n    main_path = res_block2(main_path,u1, [256, 256], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)#128x128\n    #main_path = Conv2DTranspose(128, [2, 2], strides=[2, 2])(main_path)\n    main_path2 = concatenate([main_path, from_encoder[1]], axis=3)#\n    main_path = res_block(main_path2, [128, 128], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)#256x256\n    #main_path = Conv2DTranspose(64, [2, 2], strides=[2, 2])(main_path)\n    main_path = concatenate([main_path, from_encoder[0]], axis=3)#256x256\n\n    u2 = UpSampling2D(size=(2,2))(main_path2)#\n    #u2 = Conv2DTranspose(64, [2, 2], strides=[2, 2])(main_path2)\n    u2 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u2)#\n    u2 = BatchNormalization()(u2)\n    main_path = res_block2(main_path,u2, [64, 64], [(1, 1), (1, 1)])#256x256\n\n    return main_path\n\n\ndef res2unet(lrate=7.00E-05,pretrained_weights=None):\n    print(lrate)\n    input_size=(IMG_SIZE, IMG_SIZE, 3)\n    inputs = Input(shape=input_size)\n\n    to_decoder = encoder(inputs)\n\n### Bridge\n    #path = res_block(to_decoder[3], [1024, 1024], [(2, 2), (1, 1)])####bridge\n    path = SeparableConv2D(1024, (3,3), activation = 'relu', padding = 'same')(to_decoder[-1])\n    path = SeparableConv2D(1024, (3,3), activation = 'relu', padding = 'same')(path)\n    path = BatchNormalization()(path)\n    path = MaxPooling2D(pool_size = (2,2))(path)\n    path = Dropout(0.2)(path)\n    \n    \n###  DECODER\n    path = decoder(path, from_encoder=to_decoder)\n\n\n    path = Conv2D(2, kernel_size=(3, 3),activation='relu', padding='same', strides=(1, 1))(path)\n    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path)\n    model = Model(inputs=inputs, outputs=path)\n    model.compile(optimizer=Adam(lr=lrate), loss=bce_jaccard_loss, metrics=[ACL5 ,bce_jaccard_loss ,  dice_loss,iou_coeff,precision,recall])\n    model.summary()\n    if (pretrained_weights):\n        model.load_weights(pretrained_weights)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:28.562819Z","iopub.execute_input":"2021-09-23T17:41:28.563093Z","iopub.status.idle":"2021-09-23T17:41:28.599096Z","shell.execute_reply.started":"2021-09-23T17:41:28.563060Z","shell.execute_reply":"2021-09-23T17:41:28.598437Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Even the skip features were extracted for the Decoder purposes to feed in the required Skip Features\n\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications import InceptionResNetV2\n\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\n\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications import InceptionResNetV2\n\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\n\n\ndef squeeze_excite_block(inputs, ratio=8):\n    init = inputs       ## (b, 128, 128, 32)\n    channel_axis = -1\n    filters = init.shape[channel_axis]\n    se_shape = (1, 1, filters)\n\n    se = GlobalAveragePooling2D()(init)     ## (b, 32)   -> (b, 1, 1, 32)\n    se = Reshape(se_shape)(se)\n    se = Dense(filters//ratio, activation=\"relu\", use_bias=False)(se)\n    se = Dense(filters, activation=\"sigmoid\", use_bias=False)(se)\n\n    x = Multiply()([inputs, se])\n    return x\n\ndef ASPP(x, filter):\n    shape = x.shape\n\n    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n    y1 = BatchNormalization()(y1)\n    y1 = Activation(\"relu\")(y1)\n    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n    y1 = Dropout(0.3)(y1)\n    \n    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n    y2 = BatchNormalization()(y2)\n    y2 = Activation(\"relu\")(y2)\n    \n    y2 = Dropout(0.3)(y2)\n    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n    y3 = BatchNormalization()(y3)\n    y3 = Activation(\"relu\")(y3)\n    y3 = Dropout(0.3)(y3)\n    \n    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n    y4 = BatchNormalization()(y4)\n    y4 = Activation(\"relu\")(y4)\n    y4 = Dropout(0.3)(y4)\n    \n    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n    y5 = BatchNormalization()(y5)\n    y5 = Activation(\"relu\")(y5)\n    y5 = Dropout(0.3)(y5)\n    \n    y = Concatenate()([y1, y2, y3, y4, y5])\n\n    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n    y = BatchNormalization()(y)\n    y = Activation(\"relu\")(y)\n    y= Dropout(0.2)(y)\n    \n    return y\n\ndef conv_block(x, filters):\n    x = Conv2D(filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n    \n    x = Conv2D(filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n    x = squeeze_excite_block(x)\n\n    return x\n\ndef encoder1(inputs):\n    skip_connections = []\n    model = InceptionResNetV2(include_top = False  , weights = \"imagenet\" , input_tensor = inputs)\n    #Skip Connection\n    s1 = model.get_layer(\"input_1\").output                 #Shape will be 256 * 256\n\n    s2 = model.get_layer(\"activation_2\").output         # 125 * 125 \n    s2 = ZeroPadding2D( ((2 , 1) ,  ( 2 , 1)) )(s2)          # 128 * 128\n\n\n    s3 = model.get_layer(\"activation_4\").output        # 60 * 60\n    s3 = ZeroPadding2D( ((2 , 2 ) , (2, 2)) )(s3)         # 64 * 64 \n\n\n    s4 = model.get_layer(\"activation_74\").output                              # 29*29\n    s4 = ZeroPadding2D( ((2 , 1) ,  ( 2 , 1)) )(s4) \n                                                                                # 32 * 32\n    # Padded with 2 on top , 1 in bottom     2 in left and 1 in right\n\n\n    b1 = model.get_layer(\"activation_161\").output                      # 14 * 14\n\n    #model = VGG19(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n    names = [\"input_1\", \"activation_2\", \"activation_4\", \"activation_74\"]\n\n    for feature in [s1 , s2 , s3 , s4]:\n\n        skip_connections.append(feature)\n\n\n    output = model.get_layer(\"activation_161\").output\n    output = ZeroPadding2D( ((1 , 1) , (1, 1)) )(output)                         # 16 * 16 \n\n    return output, skip_connections\n\ndef decoder1(inputs, skip_connections):\n    num_filters = [512,256, 128, 64]\n    skip_connections.reverse()\n\n    x = inputs\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2), interpolation=\"bilinear\")(x)\n        x = Concatenate()([x, skip_connections[i]])\n        x = conv_block(x, f)\n\n    return x\n\ndef output_block(inputs):\n    x = Conv2D(2, 3, padding=\"same\")(inputs)\n    x = Conv2D(1, 1, padding=\"same\")(inputs)\n    x = Activation(\"sigmoid\")(x)\n    return x\n\ndef encoder2(inputs):\n    num_filters = [ 64, 128, 256 , 512]\n    skip_connections = []\n\n    x = inputs\n\n    for i, f in enumerate(num_filters):\n        x = conv_block(x, f)\n        skip_connections.append(x)\n        x = MaxPool2D((2, 2))(x)\n\n    return x, skip_connections\n\ndef decoder2(inputs, skip_1, skip_2):\n    num_filters = [512 , 256, 128, 64]\n    skip_2.reverse()\n\n    x = inputs\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2), interpolation=\"bilinear\")(x)\n        x = Concatenate()([x, skip_1[i], skip_2[i]])\n        x = conv_block(x, f)\n\n    return x\n\n\n\n    \ndef build_model(input_shape , pretrained_weights = None):\n    \n    \n    inputs = Input(input_shape)\n    x, skip_1 = encoder1(inputs)\n    x = ASPP(x, 64)\n    x = decoder1(x, skip_1)\n    output1 = output_block(x)\n\n    x1 = inputs * output1\n    \n    \n    ## 2nd Unet\n    to_decoder = encoder(x1)\n\n### Bridge\n    #path = res_block(to_decoder[3], [1024, 1024], [(2, 2), (1, 1)])####bridge\n    path = SeparableConv2D(1024, (3,3), activation = 'relu', padding = 'same')(to_decoder[-1])\n    path = SeparableConv2D(1024, (3,3), activation = 'relu', padding = 'same')(path)\n    path = BatchNormalization()(path)\n    path = MaxPooling2D(pool_size = (2,2))(path)\n    path = Dropout(0.2)(path)\n    \n    \n###  DECODER\n    path = decoder(path, from_encoder=to_decoder)\n    \n    path = Conv2D(2, kernel_size=(3, 3),activation='relu', padding='same', strides=(1, 1))(path)\n    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path)\n    model = Model(inputs=inputs, outputs=path)\n\n    model.compile(optimizer=Adam(lr=7.00E-05), loss=bce_jaccard_loss, metrics=[ACL5 ,bce_jaccard_loss ,  dice_loss,iou_coeff,precision,recall])\n\n    model.summary()\n    if (pretrained_weights):\n        model.load_weights(pretrained_weights)\n    return model\n    \n    \n\n    \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\n#path to images\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=1)\ntest_data = dp.testGenerator()\n#model = res2unet(lrate=7.00E-05 )\n                 #pretrained_weights = \"../input/Weights/Res2Net_Conv2DTranspose (2).hdf5\" )\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\nif __name__ == \"__main__\":\n    \n    model = build_model(input_shape = ( 256 , 256 , 3) )\n    model.summary()\n    model_checkpoint1 = keras.callbacks.ModelCheckpoint('DoubleUnet.hdf5', monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\n    csv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:28.600564Z","iopub.execute_input":"2021-09-23T17:41:28.600814Z","iopub.status.idle":"2021-09-23T17:41:41.504086Z","shell.execute_reply.started":"2021-09-23T17:41:28.600782Z","shell.execute_reply":"2021-09-23T17:41:41.503358Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!pip install patchify\nmodel.load_weights(\"../input/doubleunet-res2net-bcejaccard-weights/Double_Res2UNet_BCEJaccard.hdf5\")\nfrom keras.utils import normalize\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify, unpatchify\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n\n    return x\n\ntest_path = '../input/imagestest/Images_to_Test/2'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + test_image_folder\ntest_label_folder =\"labels\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n    \n    x = trans.resize(x, (256, 256), mode='constant')\n\n    return x\n\ndef read_mask(path):\n    #x = io.imread(path, as_gray=False)\n    #x = x / 255.0\n    #x = trans.resize(x, (256, 256), mode='constant')\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, ( 256 , 256))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\n\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([256 , 256 , 3])\n  y.set_shape([256, 256, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/2/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/2/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n  \ntest_path = '../input/imagestest/Images_to_Test/1'\nfrom tqdm import tqdm\n  \n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/1/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/1/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n\nos.chdir(r'/kaggle/working')\n\nfrom zipfile import ZipFile\nimport os\nfrom os.path import basename\ndirName = \"./RESULT\"\n# create a ZipFile object\nwith ZipFile('sampleDir.zip', 'w') as zipObj:\n   # Iterate over all the files in directory\n   for folderName, subfolders, filenames in os.walk(dirName):\n       for filename in filenames:\n           #create complete filepath of file in directory\n           filePath = os.path.join(folderName, filename)\n           # Add file to zip\n           zipObj.write(filePath, basename(filePath))\n\n\ntest_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_Test\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/testdata/Test/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/testdata/Test/label'+ \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    \n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        if read_image(x).shape[-1] == 4:\n\n          x = read_image(x)[:,:,:-1]\n        else:\n          x = read_image(x)\n\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_Test/{name}.png\", image)\n\niou_score_list = []\n\nprecision_score_list = []\nrecall_score_list = []\ndice_coeff_score_list = []\n\nfor i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n    if read_image(x).shape[-1] == 4:\n\n      x = read_image(x)[:,:,:-1]\n    else:\n      x = read_image(x)\n    y = read_mask(y)\n    \n    #y_pred = model.predict(np.expand_dims(x, axis=0))\n    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n    y_pred = np.reshape(y_pred , -1)\n    y_pred = np.array([ 1.0 if values else 0.0 for values in y_pred])\n    y = y > 0.5\n    y = np.reshape(y , -1)\n    y = np.array([ 1.0 if values else 0.0 for values in y])\n    y_pred = np.reshape(y_pred , (256 ,256 , 1))\n    y = np.reshape(y , (256 ,256 , 1))\n    #y = y.astype(\"float32\")\n    #y_pred = y_pred.astype(\"float32\")\n    \n    \n    iou_score_list.append(iou_coeff(y , y_pred))\n    precision_score_list.append(precision(y , y_pred))\n    recall_score_list.append(recall(y , y_pred))\n    dice_coeff_score_list.append(dice_coeff(y , y_pred))\n\nprint(\"Average_IOU :\", np.mean(iou_score_list)) \nprint(\"Average_Precision :\", np.mean(precision_score_list)) \nprint(\"Average_Recall :\", np.mean(recall_score_list)) \nprint(\"Average_Dice_Coeff :\", np.mean(dice_coeff_score_list)) \n\nimport shutil\n\nshutil.make_archive(\"RESULT_Test\", 'zip', \"./RESULT_Test\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:41:41.505584Z","iopub.execute_input":"2021-09-23T17:41:41.505830Z","iopub.status.idle":"2021-09-23T17:44:02.574361Z","shell.execute_reply.started":"2021-09-23T17:41:41.505798Z","shell.execute_reply":"2021-09-23T17:44:02.573675Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\n\nFileLink(r'./RESULT_Test.zip')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:44:02.575650Z","iopub.execute_input":"2021-09-23T17:44:02.576047Z","iopub.status.idle":"2021-09-23T17:44:02.582589Z","shell.execute_reply.started":"2021-09-23T17:44:02.576005Z","shell.execute_reply":"2021-09-23T17:44:02.581674Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"patch_size=256\nfrom PIL import Image\nlarge_image = io.imread('../input/imagestest/Images_to_Test/1/images/i1_.png' , as_gray = False )\nlarge_image = large_image/ 255.0\nlarge_image = trans.resize(large_image, (2560, 2560), mode='constant')\n\npatches = patchify(large_image, ( 256 , 256 , 3), step= 256 )  #Step=256 for 256 patches means no overlap\n\npredicted_patches = []\nfor i in range(patches.shape[0]):\n    for j in range(patches.shape[1]):\n        #print(i,j)\n        \n        single_patch = patches[i,j,:,: ,:]   # (1 , 256 , 256 , 3)\n        \n        \n        #single_patch = single_patch/255.0\n        #single_patch_norm = np.expand_dims(np.array(single_patch), axis=1)\n        single_patch=np.expand_dims(single_patch, 0)\n        #print(single_patch.shape)\n        \n#Predict and threshold for values above 0.5 probability\n        single_patch_prediction = (model.predict(single_patch[0,:,:,:])[0] > 0.5)\n        single_patch_prediction = single_patch_prediction * 255.0\n        predicted_patches.append(single_patch_prediction)\n\npredicted_patches = np.array(predicted_patches)\n\n\npredicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 256,256) )\nreconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape[:2])\nplt.imshow(reconstructed_image, cmap='gray')\nplt.imsave('./segm1.png', reconstructed_image, cmap='gray')\n\nplt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n\nfinal_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(221)\nplt.title('Large Image')\nplt.imshow(large_image, cmap='gray')\nplt.subplot(222)\nplt.title('Prediction of large Image')\nplt.imshow(reconstructed_image, cmap='gray')\nplt.subplot(223)\nplt.title('final_prediction')\nplt.imshow(final_prediction, cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T17:44:02.583748Z","iopub.execute_input":"2021-09-23T17:44:02.584180Z","iopub.status.idle":"2021-09-23T17:44:15.562825Z","shell.execute_reply.started":"2021-09-23T17:44:02.584144Z","shell.execute_reply":"2021-09-23T17:44:15.561984Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}