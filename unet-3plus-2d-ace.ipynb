{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras==2.4.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"PNfpi6Q0jXrM","execution":{"iopub.status.busy":"2021-09-23T20:04:27.592350Z","iopub.execute_input":"2021-09-23T20:04:27.592945Z","iopub.status.idle":"2021-09-23T20:04:36.257492Z","shell.execute_reply.started":"2021-09-23T20:04:27.592850Z","shell.execute_reply":"2021-09-23T20:04:36.256676Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.4.1\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:04:36.260648Z","iopub.execute_input":"2021-09-23T20:04:36.260918Z","iopub.status.idle":"2021-09-23T20:04:42.939887Z","shell.execute_reply.started":"2021-09-23T20:04:36.260889Z","shell.execute_reply":"2021-09-23T20:04:42.939028Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\n#import keras.backend as K\n\n#%tensorflow_version 1.x\nimport os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:04:42.943157Z","iopub.execute_input":"2021-09-23T20:04:42.943398Z","iopub.status.idle":"2021-09-23T20:04:47.401820Z","shell.execute_reply.started":"2021-09-23T20:04:42.943371Z","shell.execute_reply":"2021-09-23T20:04:47.401055Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"id":"2a8AfzPOjXrQ","execution":{"iopub.status.busy":"2021-09-23T20:04:47.403779Z","iopub.execute_input":"2021-09-23T20:04:47.404052Z","iopub.status.idle":"2021-09-23T20:04:48.225012Z","shell.execute_reply.started":"2021-09-23T20:04:47.404019Z","shell.execute_reply":"2021-09-23T20:04:48.224281Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=256, img_cols=256,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=20,\n                                  width_shift_range=0.002,\n                                  shear_range=0.03,\n                                  zoom_range=0.005,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (256, 256)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        # return imgs,labels\n\n    def saveResult(self, npyfile, size, name,threshold=80):\n        for i, item in enumerate(npyfile):\n            img = item\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"id":"iTf8lIwLjXrS","execution":{"iopub.status.busy":"2021-09-23T20:04:48.229320Z","iopub.execute_input":"2021-09-23T20:04:48.231401Z","iopub.status.idle":"2021-09-23T20:04:48.742339Z","shell.execute_reply.started":"2021-09-23T20:04:48.231362Z","shell.execute_reply":"2021-09-23T20:04:48.741619Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"####  Metrics\n\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss\ndef ACL5_mod(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out*1.4)) \n\n\treturn loss","metadata":{"id":"VVdzJaSWjXra","execution":{"iopub.status.busy":"2021-09-23T20:04:48.743643Z","iopub.execute_input":"2021-09-23T20:04:48.743912Z","iopub.status.idle":"2021-09-23T20:04:48.765893Z","shell.execute_reply.started":"2021-09-23T20:04:48.743880Z","shell.execute_reply":"2021-09-23T20:04:48.765221Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iwu9rjDFjXrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_coef(y_true, y_pred, smooth=1):\n  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n  return dice\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"id":"-YuW43lejXrd","execution":{"iopub.status.busy":"2021-09-23T20:04:48.767082Z","iopub.execute_input":"2021-09-23T20:04:48.767450Z","iopub.status.idle":"2021-09-23T20:04:48.780982Z","shell.execute_reply.started":"2021-09-23T20:04:48.767412Z","shell.execute_reply":"2021-09-23T20:04:48.780189Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"id":"kr5nP8ItjXre","outputId":"48e77d7c-c144-4f17-e726-e4b36dce7171","execution":{"iopub.status.busy":"2021-09-23T20:04:48.782198Z","iopub.execute_input":"2021-09-23T20:04:48.782563Z","iopub.status.idle":"2021-09-23T20:04:48.793733Z","shell.execute_reply.started":"2021-09-23T20:04:48.782529Z","shell.execute_reply":"2021-09-23T20:04:48.793044Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"id":"KMTFbTKtjXrf","outputId":"2bd1e0fe-2f4f-479d-e8e6-b255673c5ad3","execution":{"iopub.status.busy":"2021-09-23T20:04:48.795158Z","iopub.execute_input":"2021-09-23T20:04:48.795582Z","iopub.status.idle":"2021-09-23T20:04:56.760565Z","shell.execute_reply.started":"2021-09-23T20:04:48.795550Z","shell.execute_reply":"2021-09-23T20:04:56.759807Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n\ndef focal_tversky_bce_jaccard_loss(y_true, y_pred):\n    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n","metadata":{"id":"IVcpwi4hjXrg","execution":{"iopub.status.busy":"2021-09-23T20:04:56.764079Z","iopub.execute_input":"2021-09-23T20:04:56.764348Z","iopub.status.idle":"2021-09-23T20:04:56.769555Z","shell.execute_reply.started":"2021-09-23T20:04:56.764314Z","shell.execute_reply":"2021-09-23T20:04:56.768472Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2","metadata":{"id":"eDJ9CtzQjXrh","execution":{"iopub.status.busy":"2021-09-23T20:04:56.771395Z","iopub.execute_input":"2021-09-23T20:04:56.771799Z","iopub.status.idle":"2021-09-23T20:04:56.779320Z","shell.execute_reply.started":"2021-09-23T20:04:56.771750Z","shell.execute_reply":"2021-09-23T20:04:56.778528Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Unet3Plus_ACL5","metadata":{"id":"ccYNxPZwjXri"}},{"cell_type":"code","source":"","metadata":{"id":"kFchkooujXrk","outputId":"cc29d1b8-4e58-475a-baab-26468e90c067","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\n#path to images\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=1)\ntest_data = dp.testGenerator()\nlrate = 7.00E-05 \n\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\n\n","metadata":{"id":"z73b0ddqjXrl","outputId":"a89c753e-139b-4b08-a496-19df793d5048","execution":{"iopub.status.busy":"2021-09-23T20:04:56.780413Z","iopub.execute_input":"2021-09-23T20:04:56.780868Z","iopub.status.idle":"2021-09-23T20:04:56.791674Z","shell.execute_reply.started":"2021-09-23T20:04:56.780831Z","shell.execute_reply":"2021-09-23T20:04:56.790919Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from __future__ import absolute_import\n\nfrom tensorflow import expand_dims\nfrom tensorflow.compat.v1 import image\nfrom tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D, Conv2DTranspose, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Lambda\nfrom tensorflow.keras.layers import BatchNormalization, Activation, concatenate, multiply, add\nfrom tensorflow.keras.layers import ReLU, LeakyReLU, PReLU, ELU, Softmax\n\ndef decode_layer(X, channel, pool_size, unpool, kernel_size=3, \n                 activation='ReLU', batch_norm=False, name='decode'):\n    '''\n    An overall decode layer, based on either upsampling or trans conv.\n    \n    decode_layer(X, channel, pool_size, unpool, kernel_size=3,\n                 activation='ReLU', batch_norm=False, name='decode')\n    \n    Input\n    ----------\n        X: input tensor.\n        pool_size: the decoding factor.\n        channel: (for trans conv only) number of convolution filters.\n        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n                'nearest' for Upsampling2D with nearest interpolation.\n                False for Conv2DTranspose + batch norm + activation.           \n        kernel_size: size of convolution kernels. \n                     If kernel_size='auto', then it equals to the `pool_size`.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n    \n    * The defaut: `kernel_size=3`, is suitable for `pool_size=2`.\n    \n    '''\n    # parsers\n    if unpool is False:\n        # trans conv configurations\n        bias_flag = not batch_norm\n    \n    elif unpool == 'nearest':\n        # upsample2d configurations\n        unpool = True\n        interp = 'nearest'\n    \n    elif (unpool is True) or (unpool == 'bilinear'):\n        # upsample2d configurations\n        unpool = True\n        interp = 'bilinear'\n    \n    else:\n        raise ValueError('Invalid unpool keyword')\n        \n    if unpool:\n        X = UpSampling2D(size=(pool_size, pool_size), interpolation=interp, name='{}_unpool'.format(name))(X)\n    else:\n        if kernel_size == 'auto':\n            kernel_size = pool_size\n            \n        X = Conv2DTranspose(channel, kernel_size, strides=(pool_size, pool_size), \n                            padding='same', name='{}_trans_conv'.format(name))(X)\n        \n        # batch normalization\n        if batch_norm:\n            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n            \n        # activation\n        if activation is not None:\n            activation_func = eval(activation)\n            X = activation_func(name='{}_activation'.format(name))(X)\n        X = Dropout(0.2)(X)\n    return X\n\ndef encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n                 activation='ReLU', batch_norm=False, name='encode'):\n    '''\n    An overall encode layer, based on one of the:\n    (1) max-pooling, (2) average-pooling, (3) strided conv2d.\n    \n    encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n                 activation='ReLU', batch_norm=False, name='encode')\n    \n    Input\n    ----------\n        X: input tensor.\n        pool_size: the encoding factor.\n        channel: (for strided conv only) number of convolution filters.\n        pool: True or 'max' for MaxPooling2D.\n              'ave' for AveragePooling2D.\n              False for strided conv + batch norm + activation.\n        kernel_size: size of convolution kernels. \n                     If kernel_size='auto', then it equals to the `pool_size`.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n        \n    '''\n    # parsers\n    if (pool in [False, True, 'max', 'ave']) is not True:\n        raise ValueError('Invalid pool keyword')\n        \n    # maxpooling2d as default\n    if pool is True:\n        pool = 'max'\n        \n    elif pool is False:\n        # stride conv configurations\n        bias_flag = not batch_norm\n    \n    if pool == 'max':\n        X = MaxPooling2D(pool_size=(pool_size, pool_size), name='{}_maxpool'.format(name))(X)\n        \n    elif pool == 'ave':\n        X = AveragePooling2D(pool_size=(pool_size, pool_size), name='{}_avepool'.format(name))(X)\n        \n    else:\n        if kernel_size == 'auto':\n            kernel_size = pool_size\n        \n        # linear convolution with strides\n        X = Conv2D(channel, kernel_size, strides=(pool_size, pool_size), \n                   padding='valid', use_bias=bias_flag, name='{}_stride_conv'.format(name))(X)\n        \n        # batch normalization\n        if batch_norm:\n            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n            \n        # activation\n        if activation is not None:\n            activation_func = eval(activation)\n            X = activation_func(name='{}_activation'.format(name))(X)\n        X = Dropout(0.2)(X)\n    return X\n\ndef attention_gate(X, g, channel,  \n                   activation='ReLU', \n                   attention='add', name='att'):\n    '''\n    Self-attention gate modified from Oktay et al. 2018.\n    \n    attention_gate(X, g, channel,  activation='ReLU', attention='add', name='att')\n    \n    Input\n    ----------\n        X: input tensor, i.e., key and value.\n        g: gated tensor, i.e., query.\n        channel: number of intermediate channel.\n                 Oktay et al. (2018) did not specify (denoted as F_int).\n                 intermediate channel is expected to be smaller than the input channel.\n        activation: a nonlinear attnetion activation.\n                    The `sigma_1` in Oktay et al. 2018. Default is 'ReLU'.\n        attention: 'add' for additive attention; 'multiply' for multiplicative attention.\n                   Oktay et al. 2018 applied additive attention.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X_att: output tensor.\n    \n    '''\n    activation_func = eval(activation)\n    attention_func = eval(attention)\n    \n    # mapping the input tensor to the intermediate channel\n    theta_att = Conv2D(channel, 1, use_bias=True, name='{}_theta_x'.format(name))(X)\n    \n    # mapping the gate tensor\n    phi_g = Conv2D(channel, 1, use_bias=True, name='{}_phi_g'.format(name))(g)\n    \n    # ----- attention learning ----- #\n    query = attention_func([theta_att, phi_g], name='{}_add'.format(name))\n    \n    # nonlinear activation\n    f = activation_func(name='{}_activation'.format(name))(query)\n    \n    # linear transformation\n    psi_f = Conv2D(1, 1, use_bias=True, name='{}_psi_f'.format(name))(f)\n    # ------------------------------ #\n    \n    # sigmoid activation as attention coefficients\n    coef_att = Activation('sigmoid', name='{}_sigmoid'.format(name))(psi_f)\n    \n    # multiplicative attention masking\n    X_att = multiply([X, coef_att], name='{}_masking'.format(name))\n    \n    return X_att\n\ndef CONV_stack(X, channel, kernel_size=3, stack_num=2, \n               dilation_rate=1, activation='ReLU', \n               batch_norm=False, name='conv_stack'):\n    '''\n    Stacked convolutional layers:\n    (Convolutional layer --> batch normalization --> Activation)*stack_num\n    \n    CONV_stack(X, channel, kernel_size=3, stack_num=2, dilation_rate=1, activation='ReLU', \n               batch_norm=False, name='conv_stack')\n    \n    \n    Input\n    ----------\n        X: input tensor.\n        channel: number of convolution filters.\n        kernel_size: size of 2-d convolution kernels.\n        stack_num: number of stacked Conv2D-BN-Activation layers.\n        dilation_rate: optional dilated convolution kernel.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor\n        \n    '''\n    \n    bias_flag = not batch_norm\n    \n    # stacking Convolutional layers\n    for i in range(stack_num):\n        \n        activation_func = eval(activation)\n        \n        # linear convolution\n        X = Conv2D(channel, kernel_size, padding='same', use_bias=bias_flag, \n                   dilation_rate=dilation_rate, name='{}_{}'.format(name, i))(X)\n        \n        # batch normalization\n        if batch_norm:\n            X = BatchNormalization(axis=3, name='{}_{}_bn'.format(name, i))(X)\n        \n        # activation\n        activation_func = eval(activation)\n        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\n        \n    return X\n\ndef Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv'):\n    '''\n    Stacked convolutional layers with residual path.\n     \n    Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv')\n     \n    Input\n    ----------\n        X: input tensor.\n        X_skip: the tensor that does go into the residual path \n                can be a copy of X (e.g., the identity block of ResNet).\n        channel: number of convolution filters.\n        res_num: number of convolutional layers within the residual path.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n        \n    '''  \n    X = CONV_stack(X, channel, kernel_size=3, stack_num=res_num, dilation_rate=1, \n                   activation=activation, batch_norm=batch_norm, name=name)\n\n    X = add([X_skip, X], name='{}_add'.format(name))\n    \n    activation_func = eval(activation)\n    X = activation_func(name='{}_add_activation'.format(name))(X)\n    \n    return X\n\ndef Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv'):\n    '''\n    Depthwise separable convolution with (optional) dilated convolution kernel and batch normalization.\n    \n    Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv')\n    \n    Input\n    ----------\n        X: input tensor.\n        channel: number of convolution filters.\n        kernel_size: size of 2-d convolution kernels.\n        stack_num: number of stacked depthwise-pointwise layers.\n        dilation_rate: optional dilated convolution kernel.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n    \n    '''\n    \n    activation_func = eval(activation)\n    bias_flag = not batch_norm\n    \n    for i in range(stack_num):\n        X = DepthwiseConv2D(kernel_size, dilation_rate=dilation_rate, padding='same', \n                            use_bias=bias_flag, name='{}_{}_depthwise'.format(name, i))(X)\n        \n        if batch_norm:\n            X = BatchNormalization(name='{}_{}_depthwise_BN'.format(name, i))(X)\n\n        X = activation_func(name='{}_{}_depthwise_activation'.format(name, i))(X)\n\n        X = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_{}_pointwise'.format(name, i))(X)\n        \n        if batch_norm:\n            X = BatchNormalization(name='{}_{}_pointwise_BN'.format(name, i))(X)\n\n        X = activation_func(name='{}_{}_pointwise_activation'.format(name, i))(X)\n    \n    return X\n\ndef ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp'):\n    '''\n    Atrous Spatial Pyramid Pooling (ASPP).\n    \n    ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp')\n    \n    ----------\n    Wang, Y., Liang, B., Ding, M. and Li, J., 2019. Dense semantic labeling \n    with atrous spatial pyramid pooling and decoder for high-resolution remote \n    sensing imagery. Remote Sensing, 11(1), p.20.\n    \n    Input\n    ----------\n        X: input tensor.\n        channel: number of convolution filters.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n        \n    * dilation rates are fixed to `[6, 9, 12]`.\n    '''\n    \n    activation_func = eval(activation)\n    bias_flag = not batch_norm\n\n    shape_before = X.get_shape().as_list()\n    b4 = GlobalAveragePooling2D(name='{}_avepool_b4'.format(name))(X)\n    \n    b4 = expand_dims(expand_dims(b4, 1), 1, name='{}_expdim_b4'.format(name))\n    \n    b4 = Conv2D(channel, 1, padding='same', use_bias=bias_flag, name='{}_conv_b4'.format(name))(b4)\n    \n    if batch_norm:\n        b4 = BatchNormalization(name='{}_conv_b4_BN'.format(name))(b4)\n        \n    b4 = activation_func(name='{}_conv_b4_activation'.format(name))(b4)\n    \n    # <----- tensorflow v1 resize.\n    b4 = Lambda(lambda X: image.resize(X, shape_before[1:3], method='bilinear', align_corners=True), \n                name='{}_resize_b4'.format(name))(b4)\n    \n    b0 = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_conv_b0'.format(name))(X)\n\n    if batch_norm:\n        b0 = BatchNormalization(name='{}_conv_b0_BN'.format(name))(b0)\n        \n    b0 = activation_func(name='{}_conv_b0_activation'.format(name))(b0)\n    \n    # dilation rates are fixed to `[6, 9, 12]`.\n    b_r6 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n                        dilation_rate=6, batch_norm=True, name='{}_sepconv_r6'.format(name))\n    b_r9 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n                        dilation_rate=9, batch_norm=True, name='{}_sepconv_r9'.format(name))\n    b_r12 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n                        dilation_rate=12, batch_norm=True, name='{}_sepconv_r12'.format(name))\n    \n    return concatenate([b4, b0, b_r6, b_r9, b_r12])\n\ndef CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output'):\n    '''\n    Convolutional layer with output activation.\n    \n    CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output')\n    \n    Input\n    ----------\n        X: input tensor.\n        n_labels: number of classification label(s).\n        kernel_size: size of 2-d convolution kernels. Default is 1-by-1.\n        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n                    Default option is 'Softmax'.\n                    if None is received, then linear activation is applied.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n        \n    '''\n    \n    X = Conv2D(n_labels, kernel_size, padding='same', use_bias=True, name=name)(X)\n    \n    if activation:\n        \n        if activation == 'Sigmoid':\n            X = Activation('sigmoid', name='{}_activation'.format(name))(X)\n            \n        else:\n            activation_func = eval(activation)\n            X = activation_func(name='{}_activation'.format(name))(X)\n            \n    return X","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:04:56.793820Z","iopub.execute_input":"2021-09-23T20:04:56.794098Z","iopub.status.idle":"2021-09-23T20:04:56.899212Z","shell.execute_reply.started":"2021-09-23T20:04:56.794063Z","shell.execute_reply":"2021-09-23T20:04:56.898247Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nlayer_cadidates = {\n    'VGG16': ('block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3'),\n    'VGG19': ('block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4', 'block5_conv4'),\n    'ResNet50': ('conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out'),\n    'ResNet101': ('conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block23_out', 'conv5_block3_out'),\n    'ResNet152': ('conv1_relu', 'conv2_block3_out', 'conv3_block8_out', 'conv4_block36_out', 'conv5_block3_out'),\n    'ResNet50V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block4_1_relu', 'conv4_block6_1_relu', 'post_relu'),\n    'ResNet101V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block4_1_relu', 'conv4_block23_1_relu', 'post_relu'),\n    'ResNet152V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block8_1_relu', 'conv4_block36_1_relu', 'post_relu'),\n    'DenseNet121': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n    'DenseNet169': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n    'DenseNet201': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n    'EfficientNetB0': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB1': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB2': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB3': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB4': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB5': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB6': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n    'EfficientNetB7': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),}\n\ndef bach_norm_checker(backbone_name, batch_norm):\n    '''batch norm checker'''\n    if 'VGG' in backbone_name:\n        batch_norm_backbone = False\n    else:\n        batch_norm_backbone = True\n        \n    if batch_norm_backbone != batch_norm:       \n        if batch_norm_backbone:    \n            param_mismatch = \"\\n\\nBackbone {} uses batch norm, but other layers received batch_norm={}\".format(backbone_name, batch_norm)\n        else:\n            param_mismatch = \"\\n\\nBackbone {} does not use batch norm, but other layers received batch_norm={}\".format(backbone_name, batch_norm)\n            \n        warnings.warn(param_mismatch);\n        \ndef backbone_zoo(backbone_name, weights, input_tensor, depth, freeze_backbone, freeze_batch_norm):\n    '''\n    Configuring a user specified encoder model based on the `tensorflow.keras.applications`\n    \n    Input\n    ----------\n        backbone_name: the bakcbone model name. Expected as one of the `tensorflow.keras.applications` class.\n                       Currently supported backbones are:\n                       (1) VGG16, VGG19\n                       (2) ResNet50, ResNet101, ResNet152\n                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n                       (4) DenseNet121, DenseNet169, DenseNet201\n                       (5) EfficientNetB[0,7]\n                       \n        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n                 or the path to the weights file to be loaded.\n        input_tensor: the input tensor \n        depth: number of encoded feature maps. \n               If four dwonsampling levels are needed, then depth=4.\n        \n        freeze_backbone: True for a frozen backbone\n        freeze_batch_norm: False for not freezing batch normalization layers.\n        \n    Output\n    ----------\n        model: a keras backbone model.\n        \n    '''\n    \n    cadidate = layer_cadidates[backbone_name]\n    \n    # ----- #\n    # depth checking\n    depth_max = len(cadidate)\n    if depth > depth_max:\n        depth = depth_max\n    # ----- #\n    \n    backbone_func = eval(backbone_name)\n    backbone_ = backbone_func(include_top=False, weights=weights, input_tensor=input_tensor, pooling=None,)\n    \n    X_skip = []\n    \n    for i in range(depth):\n        X_skip.append(backbone_.get_layer(cadidate[i]).output)\n        \n    model = Model(inputs=[input_tensor,], outputs=X_skip, name='{}_backbone'.format(backbone_name))\n    \n    if freeze_backbone:\n        \n        model = freeze_model(model, freeze_batch_norm=freeze_batch_norm)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:04:56.900846Z","iopub.execute_input":"2021-09-23T20:04:56.901156Z","iopub.status.idle":"2021-09-23T20:04:56.919637Z","shell.execute_reply.started":"2021-09-23T20:04:56.901115Z","shell.execute_reply":"2021-09-23T20:04:56.918675Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n              pool=True, batch_norm=False, name='left0'):\n    '''\n    The encoder block of U-net.\n    \n    UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n              pool=True, batch_norm=False, name='left0')\n    \n    Input\n    ----------\n        X: input tensor.\n        channel: number of convolution filters.\n        kernel_size: size of 2-d convolution kernels.\n        stack_num: number of convolutional layers.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n        pool: True or 'max' for MaxPooling2D.\n              'ave' for AveragePooling2D.\n              False for strided conv + batch norm + activation.\n        batch_norm: True for batch normalization, False otherwise.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n        \n    '''\n    pool_size = 2\n    \n    X = encode_layer(X, channel, pool_size, pool, activation=activation, \n                     batch_norm=batch_norm, name='{}_encode'.format(name))\n\n    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n                   batch_norm=batch_norm, name='{}_conv'.format(name))\n    \n    return X\n\n\ndef UNET_right(X, X_list, channel, kernel_size=3, \n               stack_num=2, activation='ReLU',\n               unpool=True, batch_norm=False, concat=True, name='right0'):\n    \n    '''\n    The decoder block of U-net.\n    \n    Input\n    ----------\n        X: input tensor.\n        X_list: a list of other tensors that connected to the input tensor.\n        channel: number of convolution filters.\n        kernel_size: size of 2-d convolution kernels.\n        stack_num: number of convolutional layers.\n        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n                'nearest' for Upsampling2D with nearest interpolation.\n                False for Conv2DTranspose + batch norm + activation.\n        batch_norm: True for batch normalization, False otherwise.\n        concat: True for concatenating the corresponded X_list elements.\n        name: prefix of the created keras layers.\n        \n    Output\n    ----------\n        X: output tensor.\n    \n    '''\n    \n    pool_size = 2\n    \n    X = decode_layer(X, channel, pool_size, unpool, \n                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n    \n    # linear convolutional layers before concatenation\n    X = CONV_stack(X, channel, kernel_size, stack_num=1, activation=activation, \n                   batch_norm=batch_norm, name='{}_conv_before_concat'.format(name))\n    if concat:\n        # <--- *stacked convolutional can be applied here\n        X = concatenate([X,]+X_list, axis=3, name=name+'_concat')\n    \n    # Stacked convolutions after concatenation \n    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n                   batch_norm=batch_norm, name=name+'_conv_after_concat')\n    \n    return X","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:04:56.921230Z","iopub.execute_input":"2021-09-23T20:04:56.921661Z","iopub.status.idle":"2021-09-23T20:04:56.934669Z","shell.execute_reply.started":"2021-09-23T20:04:56.921621Z","shell.execute_reply":"2021-09-23T20:04:56.933822Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"### Unet 3+ Plus\n\ndef unet_3plus_2d_base(input_tensor, filter_num_down, filter_num_skip, filter_num_aggregate, \n                       stack_num_down=2, stack_num_up=1, activation='ReLU', batch_norm=False, pool=True, unpool=True, \n                       backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus'):\n    '''\n    The base of UNET 3+ with an optional ImagNet-trained backbone.\n    \n    unet_3plus_2d_base(input_tensor, filter_num_down, filter_num_skip, filter_num_aggregate, \n                       stack_num_down=2, stack_num_up=1, activation='ReLU', batch_norm=False, pool=True, unpool=True, \n                       backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n                  \n    ----------\n    Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. \n    UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. \n    In ICASSP 2020-2020 IEEE International Conference on Acoustics, \n    Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\n    \n    Input\n    ----------\n        input_tensor: the input tensor of the base, e.g., `keras.layers.Inpyt((None, None, 3))`.        \n        filter_num_down: a list that defines the number of filters for each \n                         downsampling level. e.g., `[64, 128, 256, 512, 1024]`.\n                         the network depth is expected as `len(filter_num_down)`\n        filter_num_skip: a list that defines the number of filters after each \n                         full-scale skip connection. Number of elements is expected to be `depth-1`.\n                         i.e., the bottom level is not included.\n                         * Huang et al. (2020) applied the same numbers for all levels. \n                           e.g., `[64, 64, 64, 64]`.\n        filter_num_aggregate: an int that defines the number of channels of full-scale aggregations.\n        stack_num_down: number of convolutional layers per downsampling level/block. \n        stack_num_up: number of convolutional layers (after full-scale concat) per upsampling level/block.          \n        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., ReLU                \n        batch_norm: True for batch normalization.\n        pool: True or 'max' for MaxPooling2D.\n              'ave' for AveragePooling2D.\n              False for strided conv + batch norm + activation.\n        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n                'nearest' for Upsampling2D with nearest interpolation.\n                False for Conv2DTranspose + batch norm + activation.     \n        name: prefix of the created keras model and its layers.\n        \n        ---------- (keywords of backbone options) ----------\n        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n                       None (default) means no backbone. \n                       Currently supported backbones are:\n                       (1) VGG16, VGG19\n                       (2) ResNet50, ResNet101, ResNet152\n                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n                       (4) DenseNet121, DenseNet169, DenseNet201\n                       (5) EfficientNetB[0-7]\n        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n                 or the path to the weights file to be loaded.\n        freeze_backbone: True for a frozen backbone.\n        freeze_batch_norm: False for not freezing batch normalization layers.   \n    * Downsampling is achieved through maxpooling and can be replaced by strided convolutional layers here.\n    * Upsampling is achieved through bilinear interpolation and can be replaced by transpose convolutional layers here.\n    \n    Output\n    ----------\n        A list of tensors with the first/second/third tensor obtained from \n        the deepest/second deepest/third deepest upsampling block, etc.\n        * The feature map sizes of these tensors are different, \n          with the first tensor has the smallest size. \n    \n    '''\n    \n    depth_ = len(filter_num_down)\n\n    X_encoder = []\n    X_decoder = []\n\n    # no backbone cases\n    if backbone is None:\n\n        X = input_tensor\n        X = ASPP_conv(X, filter_num_down[0], activation='ReLU', batch_norm=True , name='{}_ASPP_conv'.format(name))\n        # stacked conv2d before downsampling\n        #X = CONV_stack(X, filter_num_down[0], kernel_size=3, stack_num=stack_num_down, \n        #              activation=activation, batch_norm=batch_norm, name='{}_down0'.format(name))\n        X_encoder.append(X)\n\n        # downsampling levels\n        for i, f in enumerate(filter_num_down[1:]):\n\n            # UNET-like downsampling\n            X = UNET_left(X, f, kernel_size=3, stack_num=stack_num_down, activation=activation, \n                          pool=pool, batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))\n            X_encoder.append(X)\n\n    else:\n        # handling VGG16 and VGG19 separately\n        if 'VGG' in backbone:\n            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n            # collecting backbone feature maps\n            X_encoder = backbone_([input_tensor,])\n            depth_encode = len(X_encoder)\n\n        # for other backbones\n        else:\n            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n            # collecting backbone feature maps\n            X_encoder = backbone_([input_tensor,])\n            depth_encode = len(X_encoder) + 1\n\n        # extra conv2d blocks are applied\n        # if downsampling levels of a backbone < user-specified downsampling levels\n        if depth_encode < depth_:\n\n            # begins at the deepest available tensor  \n            X = X_encoder[-1]\n\n            # extra downsamplings\n            for i in range(depth_-depth_encode):\n\n                i_real = i + depth_encode\n\n                X = UNET_left(X, filter_num_down[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n                X_encoder.append(X)\n\n\n    # treat the last encoded tensor as the first decoded tensor\n    X_decoder.append(X_encoder[-1])\n\n    # upsampling levels\n    X_encoder = X_encoder[::-1]\n\n    depth_decode = len(X_encoder)-1\n\n    # loop over upsampling levels\n    for i in range(depth_decode):\n\n        f = filter_num_skip[i]\n\n        # collecting tensors for layer fusion\n        X_fscale = []\n\n        # for each upsampling level, loop over all available downsampling levels (similar to the unet++)\n        for lev in range(depth_decode):\n\n            # counting scale difference between the current down- and upsampling levels\n            pool_scale = lev-i-1 # -1 for python indexing\n\n            # deeper tensors are obtained from **decoder** outputs\n            if pool_scale < 0:\n                pool_size = 2**(-1*pool_scale)\n                \n                X = decode_layer(X_decoder[lev], f, pool_size, unpool, \n                     activation=activation, batch_norm=batch_norm, name='{}_up_{}_en{}'.format(name, i, lev))\n\n            # unet skip connection (identity mapping)    \n            elif pool_scale == 0:\n\n                X = X_encoder[lev]\n\n            # shallower tensors are obtained from **encoder** outputs\n            else:\n                pool_size = 2**(pool_scale)\n                \n                X = encode_layer(X_encoder[lev], f, pool_size, pool, activation=activation, \n                                 batch_norm=batch_norm, name='{}_down_{}_en{}'.format(name, i, lev))\n\n            # a conv layer after feature map scale change\n            #X = CONV_stack(X, f, kernel_size=3, stack_num=1, \n            #               activation=activation, batch_norm=batch_norm, name='{}_down_from{}_to{}'.format(name, i, lev))\n            X = ASPP_conv(X, f , activation='ReLU', batch_norm=True , name='{}_down_from{}_to{}'.format(name, i, lev))\n            X_fscale.append(X)  \n\n        # layer fusion at the end of each level\n        # stacked conv layers after concat. BatchNormalization is fixed to True\n\n        X = concatenate(X_fscale, axis=-1, name='{}_concat_{}'.format(name, i))\n        X = CONV_stack(X, filter_num_aggregate, kernel_size=3, stack_num=stack_num_up, \n                       activation=activation, batch_norm=True, name='{}_fusion_conv_{}'.format(name, i))\n        X_decoder.append(X)\n\n    # if tensors for concatenation is not enough\n    # then use upsampling without concatenation \n    if depth_decode < depth_-1:\n        for i in range(depth_-depth_decode-1):\n            i_real = i + depth_decode\n            X = UNET_right(X, None, filter_num_aggregate, stack_num=stack_num_up, activation=activation, \n                           unpool=unpool, batch_norm=batch_norm, concat=False, name='{}_plain_up{}'.format(name, i_real))\n            X_decoder.append(X)\n        \n    # return decoder outputs\n    return X_decoder\n\ndef unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip='auto', filter_num_aggregate='auto', \n                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='sigmoid',\n                  batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n                  backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus'):\n    \n    '''\n    UNET 3+ with an optional ImageNet-trained backbone.\n    \n    unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip='auto', filter_num_aggregate='auto', \n                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n                  batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n                  backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n                  \n    ----------\n    Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. \n    UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. \n    In ICASSP 2020-2020 IEEE International Conference on Acoustics, \n    Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\n    \n    Input\n    ----------\n        input_size: the size/shape of network input, e.g., `(128, 128, 3)`.\n        filter_num_down: a list that defines the number of filters for each \n                         downsampling level. e.g., `[64, 128, 256, 512, 1024]`.\n                         the network depth is expected as `len(filter_num_down)`\n        filter_num_skip: a list that defines the number of filters after each \n                         full-scale skip connection. Number of elements is expected to be `depth-1`.\n                         i.e., the bottom level is not included.\n                         * Huang et al. (2020) applied the same numbers for all levels. \n                           e.g., `[64, 64, 64, 64]`.\n        filter_num_aggregate: an int that defines the number of channels of full-scale aggregations.\n        stack_num_down: number of convolutional layers per downsampling level/block. \n        stack_num_up: number of convolutional layers (after full-scale concat) per upsampling level/block.\n        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'\n        output_activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n                           Default option is 'Softmax'.\n                           if None is received, then linear activation is applied.\n        batch_norm: True for batch normalization.\n        pool: True or 'max' for MaxPooling2D.\n              'ave' for AveragePooling2D.\n              False for strided conv + batch norm + activation.\n        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n                'nearest' for Upsampling2D with nearest interpolation.\n                False for Conv2DTranspose + batch norm + activation.   \n        deep_supervision: True for a model that supports deep supervision. Details see Huang et al. (2020).\n        name: prefix of the created keras model and its layers.\n        \n        ---------- (keywords of backbone options) ----------\n        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n                       None (default) means no backbone. \n                       Currently supported backbones are:\n                       (1) VGG16, VGG19\n                       (2) ResNet50, ResNet101, ResNet152\n                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n                       (4) DenseNet121, DenseNet169, DenseNet201\n                       (5) EfficientNetB[0-7]\n        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n                 or the path to the weights file to be loaded.\n        freeze_backbone: True for a frozen backbone.\n        freeze_batch_norm: False for not freezing batch normalization layers.   \n        \n    * The Classification-guided Module (CGM) is not implemented. \n      See https://github.com/yingkaisha/keras-unet-collection/tree/main/examples for a relevant example.\n    * Automated mode is applied for determining `filter_num_skip`, `filter_num_aggregate`.\n    * The default output activation is sigmoid, consistent with Huang et al. (2020).\n    * Downsampling is achieved through maxpooling and can be replaced by strided convolutional layers here.\n    * Upsampling is achieved through bilinear interpolation and can be replaced by transpose convolutional layers here.\n    \n    Output\n    ----------\n        model: a keras model.\n    \n    '''\n\n    depth_ = len(filter_num_down)\n    \n    verbose = False\n    \n    if filter_num_skip == 'auto':\n        verbose = True\n        filter_num_skip = [filter_num_down[0] for num in range(depth_-1)]\n        \n    if filter_num_aggregate == 'auto':\n        verbose = True\n        filter_num_aggregate = int(depth_*filter_num_down[0])\n        \n    if verbose:\n        print('Automated hyper-parameter determination is applied with the following details:\\n----------')\n        print('\\tNumber of convolution filters after each full-scale skip connection: filter_num_skip = {}'.format(filter_num_skip))\n        print('\\tNumber of channels of full-scale aggregated feature maps: filter_num_aggregate = {}'.format(filter_num_aggregate))    \n    \n    if backbone is not None:\n        bach_norm_checker(backbone, batch_norm)\n    \n    X_encoder = []\n    X_decoder = []\n\n\n    IN = Input(input_size)\n\n    X_decoder = unet_3plus_2d_base(IN, filter_num_down, filter_num_skip, filter_num_aggregate, \n                                   stack_num_down=stack_num_down, stack_num_up=stack_num_up, activation=activation, \n                                   batch_norm=batch_norm, pool=pool, unpool=unpool, \n                                   backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, \n                                   freeze_batch_norm=freeze_batch_norm, name=name)\n    X_decoder = X_decoder[::-1]\n\n    if deep_supervision:\n        \n        # ----- frozen backbone issue checker ----- #\n        if ('{}_backbone_'.format(backbone) in X_decoder[0].name) and freeze_backbone:\n            \n            backbone_warn = '\\n\\nThe deepest UNET 3+ deep supervision branch directly connects to a frozen backbone.\\nTesting your configurations on `keras_unet_collection.base.unet_plus_2d_base` is recommended.'\n            warnings.warn(backbone_warn);\n        # ----------------------------------------- #\n        \n        OUT_stack = []\n        L_out = len(X_decoder)\n        \n        print('----------\\ndeep_supervision = True\\nnames of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\\n\"final\" is the final output layer):\\n')\n        \n        # conv2d --> upsampling --> output activation.\n        # index 0 is final output \n        for i in range(1, L_out):\n            \n            pool_size = 2**(i)\n            \n            X = Conv2D(n_labels, 3, padding='same', name='{}_output_conv_{}'.format(name, i-1))(X_decoder[i])\n            \n            X = decode_layer(X, n_labels, pool_size, unpool, \n                             activation=None, batch_norm=False, name='{}_output_sup{}'.format(name, i-1))\n            \n            if output_activation:\n                print('\\t{}_output_sup{}_activation'.format(name, i-1))\n                \n                if output_activation == 'Sigmoid':\n                    X = Activation('sigmoid', name='{}_output_sup{}_activation'.format(name, i-1))(X)\n                else:\n                    activation_func = eval(output_activation)\n                    X = activation_func(name='{}_output_sup{}_activation'.format(name, i-1))(X)\n            else:\n                if unpool is False:\n                    print('\\t{}_output_sup{}_trans_conv'.format(name, i-1))\n                else:\n                    print('\\t{}_output_sup{}_unpool'.format(name, i-1))\n                    \n            OUT_stack.append(X)\n        \n        X = CONV_output(X_decoder[0], n_labels, kernel_size=3, \n                        activation=output_activation, name='{}_output_final'.format(name))\n        OUT_stack.append(X)\n        \n        \n        concat_out = concatenate([OUT_stack[0],OUT_stack[1] , OUT_stack[2] , OUT_stack[4]], axis=-1)\n        \n        X = CONV_output(concat_out, n_labels, kernel_size=3, \n                        activation=output_activation, name='{}concate_output1'.format(name))\n        OUT_stack.append(X)        \n        \n        if output_activation:\n            print('\\t{}_output_final_activation'.format(name))\n        else:\n            print('\\t{}_output_final'.format(name))\n            \n        model = Model([IN,], OUT_stack)\n\n    else:\n        OUT = CONV_output(X_decoder[0], n_labels, kernel_size=3, \n                          activation=output_activation, name='{}_output_final'.format(name))\n\n        model = Model([IN,], [OUT,])\n        \n    return model\n\n\n\ndef freeze_model(model, freeze_batch_norm=False):\n    '''\n    freeze a keras model\n    \n    Input\n    ----------\n        model: a keras model\n        freeze_batch_norm: False for not freezing batch notmalization layers\n    '''\n    if freeze_batch_norm:\n        for layer in model.layers:\n            layer.trainable = False\n    else:\n        from tensorflow.keras.layers import BatchNormalization    \n        for layer in model.layers:\n            if isinstance(layer, BatchNormalization):\n                layer.trainable = True\n            else:\n                layer.trainable = False\n    return model\nif __name__ == \"__main__\":\n    \n    name = 'unet3plus'\n    activation = 'ReLU'\n    filter_num_down = [32, 64, 128, 256, 512 , 1024]\n    filter_num_skip = [64, 64, 64, 64 , 64]\n    filter_num_aggregate = 160\n\n    stack_num_down = 2\n    stack_num_up = 1\n    n_labels = 1\n    input_shape = (256, 256, 3)\n    \n    \n    model = unet_3plus_2d(input_shape , n_labels, filter_num_down, filter_num_skip=filter_num_skip, filter_num_aggregate=filter_num_aggregate, \n                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n                  batch_norm=True, pool=True, unpool=False, deep_supervision=False, \n                  backbone= \"DenseNet201\", weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n    model.summary()\n    model_checkpoint1 = keras.callbacks.ModelCheckpoint('unet3Plus_ACL.hdf5', monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\n    csv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\n    model.compile(optimizer=Adam(lr=lrate), loss=ACL5 , metrics=[ACL5 ,bce_jaccard_loss , dice_coef , dsc,  dice_loss,iou_coeff,precision,recall])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:04:56.936382Z","iopub.execute_input":"2021-09-23T20:04:56.936822Z","iopub.status.idle":"2021-09-23T20:05:08.029299Z","shell.execute_reply.started":"2021-09-23T20:04:56.936786Z","shell.execute_reply":"2021-09-23T20:05:08.028514Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit_generator(train_data,\n#                              steps_per_epoch=1912,epochs=40,\n#                              validation_steps=207,\n#                              validation_data=valid_data,\n#                              callbacks=[model_checkpoint1,csv_logger])","metadata":{"id":"DJLOnaYejXrm","outputId":"deabad6b-2cf2-449a-8124-ea3bd1e2a072","execution":{"iopub.status.busy":"2021-09-23T20:05:08.030726Z","iopub.execute_input":"2021-09-23T20:05:08.030992Z","iopub.status.idle":"2021-09-23T20:05:08.036893Z","shell.execute_reply.started":"2021-09-23T20:05:08.030958Z","shell.execute_reply":"2021-09-23T20:05:08.036048Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Predictation on Some Images","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"../input/unet3plus-acl-weights/unet3Plus_ACL.hdf5\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:05:08.038224Z","iopub.execute_input":"2021-09-23T20:05:08.038473Z","iopub.status.idle":"2021-09-23T20:05:11.899061Z","shell.execute_reply.started":"2021-09-23T20:05:08.038441Z","shell.execute_reply":"2021-09-23T20:05:11.898296Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"!pip install patchify\nfrom keras.utils import normalize\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify, unpatchify\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n\n    return x\n\ntest_path = '../input/imagestest/Images_to_Test/2'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + test_image_folder\ntest_label_folder =\"labels\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n    \n    x = trans.resize(x, (256, 256), mode='constant')\n\n    return x\n\ndef read_mask(path):\n    #x = io.imread(path, as_gray=False)\n    #x = x / 255.0\n    #x = trans.resize(x, (256, 256), mode='constant')\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, ( 256 , 256))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\n\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([256 , 256 , 3])\n  y.set_shape([256, 256, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/2/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/2/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n  \ntest_path = '../input/imagestest/Images_to_Test/1'\nfrom tqdm import tqdm\n  \n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/1/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/1/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n\nos.chdir(r'/kaggle/working')\n\nfrom zipfile import ZipFile\nimport os\nfrom os.path import basename\ndirName = \"./RESULT\"\n# create a ZipFile object\nwith ZipFile('sampleDir.zip', 'w') as zipObj:\n   # Iterate over all the files in directory\n   for folderName, subfolders, filenames in os.walk(dirName):\n       for filename in filenames:\n           #create complete filepath of file in directory\n           filePath = os.path.join(folderName, filename)\n           # Add file to zip\n           zipObj.write(filePath, basename(filePath))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:05:11.900645Z","iopub.execute_input":"2021-09-23T20:05:11.900937Z","iopub.status.idle":"2021-09-23T20:05:36.270878Z","shell.execute_reply.started":"2021-09-23T20:05:11.900906Z","shell.execute_reply":"2021-09-23T20:05:36.269740Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_Test\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/testdata/Test/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/testdata/Test/label'+ \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    \n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        if read_image(x).shape[-1] == 4:\n\n          x = read_image(x)[:,:,:-1]\n        else:\n          x = read_image(x)\n\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_Test/{name}.png\", image)\n\niou_score_list = []\n\nprecision_score_list = []\nrecall_score_list = []\ndice_coeff_score_list = []\n\nfor i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n    if read_image(x).shape[-1] == 4:\n\n      x = read_image(x)[:,:,:-1]\n    else:\n      x = read_image(x)\n    y = read_mask(y)\n    \n    #y_pred = model.predict(np.expand_dims(x, axis=0))\n    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n    y_pred = np.reshape(y_pred , -1)\n    y_pred = np.array([ 1.0 if values else 0.0 for values in y_pred])\n    y = y > 0.5\n    y = np.reshape(y , -1)\n    y = np.array([ 1.0 if values else 0.0 for values in y])\n    y_pred = np.reshape(y_pred , (256 ,256 , 1))\n    y = np.reshape(y , (256 ,256 , 1))\n    #y = y.astype(\"float32\")\n    #y_pred = y_pred.astype(\"float32\")\n    \n    \n    iou_score_list.append(iou_coeff(y , y_pred))\n    precision_score_list.append(precision(y , y_pred))\n    recall_score_list.append(recall(y , y_pred))\n    dice_coeff_score_list.append(dice_coeff(y , y_pred))\n\nprint(\"Average_IOU :\", np.mean(iou_score_list)) \nprint(\"Average_Precision :\", np.mean(precision_score_list)) \nprint(\"Average_Recall :\", np.mean(recall_score_list)) \nprint(\"Average_Dice_Coeff :\", np.mean(dice_coeff_score_list)) \n\nimport shutil\n\nshutil.make_archive(\"RESULT_Test\", 'zip', \"./RESULT_Test\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:05:36.272593Z","iopub.execute_input":"2021-09-23T20:05:36.273142Z","iopub.status.idle":"2021-09-23T20:07:39.665104Z","shell.execute_reply.started":"2021-09-23T20:05:36.273102Z","shell.execute_reply":"2021-09-23T20:07:39.664381Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\n\nFileLink(r'./RESULT_Test.zip')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:07:39.666406Z","iopub.execute_input":"2021-09-23T20:07:39.666741Z","iopub.status.idle":"2021-09-23T20:07:39.673794Z","shell.execute_reply.started":"2021-09-23T20:07:39.666705Z","shell.execute_reply":"2021-09-23T20:07:39.672853Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"patch_size=256\nfrom PIL import Image\nlarge_image = io.imread('../input/imagestest/Images_to_Test/1/images/i1_.png' , as_gray = False )\nlarge_image = large_image/ 255.0\nlarge_image = trans.resize(large_image, (2560, 2560), mode='constant')\n\npatches = patchify(large_image, ( 256 , 256 , 3), step= 256 )  #Step=256 for 256 patches means no overlap\n\npredicted_patches = []\nfor i in range(patches.shape[0]):\n    for j in range(patches.shape[1]):\n        #print(i,j)\n        \n        single_patch = patches[i,j,:,: ,:]   # (1 , 256 , 256 , 3)\n        \n        \n        #single_patch = single_patch/255.0\n        #single_patch_norm = np.expand_dims(np.array(single_patch), axis=1)\n        single_patch=np.expand_dims(single_patch, 0)\n        #print(single_patch.shape)\n        \n#Predict and threshold for values above 0.5 probability\n        single_patch_prediction = (model.predict(single_patch[0,:,:,:])[0] > 0.5)\n        single_patch_prediction = single_patch_prediction * 255.0\n        predicted_patches.append(single_patch_prediction)\n\npredicted_patches = np.array(predicted_patches)\n\n\npredicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 256,256) )\nreconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape[:2])\nplt.imshow(reconstructed_image, cmap='gray')\nplt.imsave('./segm1.png', reconstructed_image, cmap='gray')\n\nplt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n\nfinal_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(221)\nplt.title('Large Image')\nplt.imshow(large_image, cmap='gray')\nplt.subplot(222)\nplt.title('Prediction of large Image')\nplt.imshow(reconstructed_image, cmap='gray')\nplt.subplot(223)\nplt.title('final_prediction')\nplt.imshow(final_prediction, cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T20:07:39.675314Z","iopub.execute_input":"2021-09-23T20:07:39.675692Z","iopub.status.idle":"2021-09-23T20:07:53.546408Z","shell.execute_reply.started":"2021-09-23T20:07:39.675654Z","shell.execute_reply":"2021-09-23T20:07:53.545758Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# UNet3Plus on Test Data","metadata":{"id":"nR7LOc44jXro"}},{"cell_type":"code","source":"","metadata":{"id":"y84NzOdzjXrp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"9zuUHlYwjXrs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"JcROpUrFjXrs","outputId":"e2364a58-083a-4e93-8e00-6690beee2c1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"bYd43FdDjXrt","outputId":"2771ab44-fe22-40fb-baff-4b674f4b1312","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"hjfMb9BnjXrt","outputId":"69b96855-6e0e-42f1-f143-e37f10197229","trusted":true},"execution_count":null,"outputs":[]}]}