{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras==2.4.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"PNfpi6Q0jXrM","execution":{"iopub.status.busy":"2021-09-23T18:19:57.602632Z","iopub.execute_input":"2021-09-23T18:19:57.603158Z","iopub.status.idle":"2021-09-23T18:20:07.382461Z","shell.execute_reply.started":"2021-09-23T18:19:57.603066Z","shell.execute_reply":"2021-09-23T18:20:07.381581Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.4.1\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T18:20:07.386222Z","iopub.execute_input":"2021-09-23T18:20:07.386823Z","iopub.status.idle":"2021-09-23T18:20:13.996448Z","shell.execute_reply.started":"2021-09-23T18:20:07.386771Z","shell.execute_reply":"2021-09-23T18:20:13.995636Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\n#import keras.backend as K\n\n#%tensorflow_version 1.x\nimport os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger","metadata":{"execution":{"iopub.status.busy":"2021-09-23T18:20:13.999569Z","iopub.execute_input":"2021-09-23T18:20:13.999808Z","iopub.status.idle":"2021-09-23T18:20:18.482890Z","shell.execute_reply.started":"2021-09-23T18:20:13.999774Z","shell.execute_reply":"2021-09-23T18:20:18.482183Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"id":"2a8AfzPOjXrQ","execution":{"iopub.status.busy":"2021-09-23T18:20:18.483979Z","iopub.execute_input":"2021-09-23T18:20:18.484210Z","iopub.status.idle":"2021-09-23T18:20:19.190059Z","shell.execute_reply.started":"2021-09-23T18:20:18.484181Z","shell.execute_reply":"2021-09-23T18:20:19.189200Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=256, img_cols=256,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=20,\n                                  width_shift_range=0.002,\n                                  shear_range=0.03,\n                                  zoom_range=0.005,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (256, 256)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        # return imgs,labels\n\n    def saveResult(self, npyfile, size, name,threshold=80):\n        for i, item in enumerate(npyfile):\n            img = item\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"id":"iTf8lIwLjXrS","execution":{"iopub.status.busy":"2021-09-23T18:20:19.192853Z","iopub.execute_input":"2021-09-23T18:20:19.193137Z","iopub.status.idle":"2021-09-23T18:20:19.694366Z","shell.execute_reply.started":"2021-09-23T18:20:19.193102Z","shell.execute_reply":"2021-09-23T18:20:19.693690Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"####  Metrics\n\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss\ndef ACL5_mod(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out*1.4)) \n\n\treturn loss","metadata":{"id":"VVdzJaSWjXra","execution":{"iopub.status.busy":"2021-09-23T18:20:19.695842Z","iopub.execute_input":"2021-09-23T18:20:19.696147Z","iopub.status.idle":"2021-09-23T18:20:19.719216Z","shell.execute_reply.started":"2021-09-23T18:20:19.696108Z","shell.execute_reply":"2021-09-23T18:20:19.718219Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iwu9rjDFjXrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_coef(y_true, y_pred, smooth=1):\n  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n  return dice\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"id":"-YuW43lejXrd","execution":{"iopub.status.busy":"2021-09-23T18:20:19.720456Z","iopub.execute_input":"2021-09-23T18:20:19.721183Z","iopub.status.idle":"2021-09-23T18:20:19.735173Z","shell.execute_reply.started":"2021-09-23T18:20:19.721148Z","shell.execute_reply":"2021-09-23T18:20:19.734466Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"id":"kr5nP8ItjXre","outputId":"48e77d7c-c144-4f17-e726-e4b36dce7171","execution":{"iopub.status.busy":"2021-09-23T18:20:19.736624Z","iopub.execute_input":"2021-09-23T18:20:19.736923Z","iopub.status.idle":"2021-09-23T18:20:19.748949Z","shell.execute_reply.started":"2021-09-23T18:20:19.736890Z","shell.execute_reply":"2021-09-23T18:20:19.747880Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"id":"KMTFbTKtjXrf","outputId":"2bd1e0fe-2f4f-479d-e8e6-b255673c5ad3","execution":{"iopub.status.busy":"2021-09-23T18:20:19.750332Z","iopub.execute_input":"2021-09-23T18:20:19.750657Z","iopub.status.idle":"2021-09-23T18:20:28.703465Z","shell.execute_reply.started":"2021-09-23T18:20:19.750601Z","shell.execute_reply":"2021-09-23T18:20:28.702694Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n\ndef focal_tversky_bce_jaccard_loss(y_true, y_pred):\n    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n","metadata":{"id":"IVcpwi4hjXrg","execution":{"iopub.status.busy":"2021-09-23T18:20:28.705181Z","iopub.execute_input":"2021-09-23T18:20:28.705443Z","iopub.status.idle":"2021-09-23T18:20:28.710558Z","shell.execute_reply.started":"2021-09-23T18:20:28.705408Z","shell.execute_reply":"2021-09-23T18:20:28.709806Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2","metadata":{"id":"eDJ9CtzQjXrh","execution":{"iopub.status.busy":"2021-09-23T18:20:28.712047Z","iopub.execute_input":"2021-09-23T18:20:28.712633Z","iopub.status.idle":"2021-09-23T18:20:28.721257Z","shell.execute_reply.started":"2021-09-23T18:20:28.712583Z","shell.execute_reply":"2021-09-23T18:20:28.720437Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Attention_Residual2Unet","metadata":{"id":"ccYNxPZwjXri"}},{"cell_type":"code","source":"","metadata":{"id":"kFchkooujXrk","outputId":"cc29d1b8-4e58-475a-baab-26468e90c067","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\n#path to images\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=1)\ntest_data = dp.testGenerator()\nlrate = 7.00E-05 \n\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\n\n","metadata":{"id":"z73b0ddqjXrl","outputId":"a89c753e-139b-4b08-a496-19df793d5048","execution":{"iopub.status.busy":"2021-09-23T18:20:28.722729Z","iopub.execute_input":"2021-09-23T18:20:28.722998Z","iopub.status.idle":"2021-09-23T18:20:28.733980Z","shell.execute_reply.started":"2021-09-23T18:20:28.722966Z","shell.execute_reply":"2021-09-23T18:20:28.733314Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\nimport keras.backend as K\n\n\n\n\ndef up_and_concate(down_layer, layer, data_format='channels_first'):\n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n\n    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n\n    concate = my_concat([up, layer])\n\n    return concate\n\n\ndef attention_up_and_concate(down_layer, layer, data_format='channels_first'):\n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n\n    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n\n    layer = attention_block_2d(x=layer, g=up, inter_channel=in_channel // 4, data_format=data_format)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n\n    concate = my_concat([up, layer])\n    return concate\n\n\ndef attention_block_2d(x, g, inter_channel, data_format='channels_first'):\n    # theta_x(?,g_height,g_width,inter_channel)\n\n    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(x)\n\n    # phi_g(?,g_height,g_width,inter_channel)\n\n    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(g)\n\n    # f(?,g_height,g_width,inter_channel)\n\n    f = Activation('relu')(add([theta_x, phi_g]))\n\n    # psi_f(?,g_height,g_width,1)\n\n    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format=data_format)(f)\n\n    rate = Activation('sigmoid')(psi_f)\n\n    # rate(?,x_height,x_width)\n\n    # att_x(?,x_height,x_width,x_channel)\n\n    att_x = multiply([x, rate])\n\n    return att_x\n\n\ndef res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n\n              padding='same', data_format='channels_first'):\n    if data_format == 'channels_first':\n        input_n_filters = input_layer.get_shape().as_list()[1]\n    else:\n        input_n_filters = input_layer.get_shape().as_list()[3]\n\n    layer = input_layer\n    for i in range(2):\n        layer = Conv2D(out_n_filters // 4, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n        if batch_normalization:\n            layer = BatchNormalization()(layer)\n        layer = Activation('relu')(layer)\n        layer = Conv2D(out_n_filters // 4, kernel_size, strides=stride, padding=padding, data_format=data_format)(layer)\n        layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n\n    if out_n_filters != input_n_filters:\n        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n            input_layer)\n    else:\n        skip_layer = input_layer\n    out_layer = add([layer, skip_layer])\n    return out_layer\n\n\n# Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\ndef rec_res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n\n                  padding='same', data_format='channels_first'):\n    if data_format == 'channels_first':\n        input_n_filters = input_layer.get_shape().as_list()[1]\n    else:\n        input_n_filters = input_layer.get_shape().as_list()[3]\n\n    if out_n_filters != input_n_filters:\n        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n            input_layer)\n    else:\n        skip_layer = input_layer\n\n    layer = skip_layer\n    for j in range(2):\n\n        for i in range(2):\n            if i == 0:\n\n                layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n                    layer)\n                if batch_normalization:\n                    layer1 = BatchNormalization()(layer1)\n                layer1 = Activation('relu')(layer1)\n            layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n                add([layer1, layer]))\n            if batch_normalization:\n                layer1 = BatchNormalization()(layer1)\n            layer1 = Activation('relu')(layer1)\n        layer = layer1\n\n    out_layer = add([layer, skip_layer])\n    return out_layer\n\n########################################################################################################\n# Define the neural network\ndef unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format= data_format)(x)\n        features = features * 2\n\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        # attention_up_and_concate(x,[skips[i])\n        x = UpSampling2D(size=(2, 2), data_format=data_format)(x)\n        x = concatenate([skips[i], x], axis=1)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n\n    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Attention U-Net\ndef att_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format='channels_first')(x)\n        features = features * 2\n\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n\n    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\ndef r2_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = rec_res_block(x, features, data_format=data_format)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n\n        features = features * 2\n\n    x = rec_res_block(x, features, data_format=data_format)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = up_and_concate(x, skips[i], data_format=data_format)\n        x = rec_res_block(x, features, data_format=data_format)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n    #model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Attention R2U-Net\ndef att_r2_unet(img_w, img_h, n_label, data_format='channels_last'):\n    inputs = Input((img_w, img_h , 3))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = rec_res_block(x, features, data_format=data_format)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n\n        features = features * 2\n\n    x = rec_res_block(x, features, data_format=data_format)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = rec_res_block(x, features, data_format=data_format)\n        \n    x = Conv2D( 2 , (3, 3), padding='same', data_format=data_format)(x)\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n    return model\n\n\nif __name__ == \"__main__\":\n    input_shape = (256, 256, 3)\n    model = att_r2_unet(256, 256, 1)\n    model.summary()\n    model_checkpoint1 = keras.callbacks.ModelCheckpoint('att_r2_unet.hdf5', monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\n    csv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\n    model.compile(optimizer=Adam(lr=lrate), loss=bce_jaccard_loss , metrics=[ACL5 ,bce_jaccard_loss , dice_coef , dsc,  dice_loss,iou_coeff,precision,recall])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T18:20:28.735529Z","iopub.execute_input":"2021-09-23T18:20:28.735973Z","iopub.status.idle":"2021-09-23T18:20:31.851332Z","shell.execute_reply.started":"2021-09-23T18:20:28.735939Z","shell.execute_reply":"2021-09-23T18:20:31.850629Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Attention_Residual2Unet on Test Data","metadata":{"id":"nR7LOc44jXro"}},{"cell_type":"code","source":"model.load_weights(\"../input/attention-r2-unet-bcejaccard-weights/Attention_R2_Unet.hdf5\")","metadata":{"id":"y84NzOdzjXrp","execution":{"iopub.status.busy":"2021-09-23T18:20:31.854385Z","iopub.execute_input":"2021-09-23T18:20:31.854585Z","iopub.status.idle":"2021-09-23T18:20:37.310996Z","shell.execute_reply.started":"2021-09-23T18:20:31.854562Z","shell.execute_reply":"2021-09-23T18:20:37.310226Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!pip install patchify\nfrom keras.utils import normalize\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify, unpatchify\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n\n    return x\n\ntest_path = '../input/imagestest/Images_to_Test/2'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + test_image_folder\ntest_label_folder =\"labels\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\ndef read_image(path):\n    x = io.imread(path, as_gray=False)\n    x = x / 255.0\n    \n    x = trans.resize(x, (256, 256), mode='constant')\n\n    return x\n\ndef read_mask(path):\n    #x = io.imread(path, as_gray=False)\n    #x = x / 255.0\n    #x = trans.resize(x, (256, 256), mode='constant')\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, ( 256 , 256))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\n\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([256 , 256 , 3])\n  y.set_shape([256, 256, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/2/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/2/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n  \ntest_path = '../input/imagestest/Images_to_Test/1'\nfrom tqdm import tqdm\n  \n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/imagestest/Images_to_Test/1/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/imagestest/Images_to_Test/1/labels' + \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        x = read_image(x)\n        y = read_mask(y)\n        #model.load_weights(\"./Double_Res2UNet.hdf5\")\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT/{name}.png\", image)\n\nos.chdir(r'/kaggle/working')\n\nfrom zipfile import ZipFile\nimport os\nfrom os.path import basename\ndirName = \"./RESULT\"\n# create a ZipFile object\nwith ZipFile('sampleDir.zip', 'w') as zipObj:\n   # Iterate over all the files in directory\n   for folderName, subfolders, filenames in os.walk(dirName):\n       for filename in filenames:\n           #create complete filepath of file in directory\n           filePath = os.path.join(folderName, filename)\n           # Add file to zip\n           zipObj.write(filePath, basename(filePath))\n\n\ntest_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_Test\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n\nfrom tqdm import tqdm\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/testdata/Test/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/testdata/Test/label'+ \"/\" +label_file_name)\n    test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    test_steps = (len(TEST_X)//batch_size)\n    if len(TEST_X) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n    \n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        name = x.split(\".\")[2].split(\"/\")[-1]\n        if read_image(x).shape[-1] == 4:\n\n          x = read_image(x)[:,:,:-1]\n        else:\n          x = read_image(x)\n\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_Test/{name}.png\", image)\n\niou_score_list = []\n\nprecision_score_list = []\nrecall_score_list = []\ndice_coeff_score_list = []\n\nfor i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n    if read_image(x).shape[-1] == 4:\n\n      x = read_image(x)[:,:,:-1]\n    else:\n      x = read_image(x)\n    y = read_mask(y)\n    \n    #y_pred = model.predict(np.expand_dims(x, axis=0))\n    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n    y_pred = np.reshape(y_pred , -1)\n    y_pred = np.array([ 1.0 if values else 0.0 for values in y_pred])\n    y = y > 0.5\n    y = np.reshape(y , -1)\n    y = np.array([ 1.0 if values else 0.0 for values in y])\n    y_pred = np.reshape(y_pred , (256 ,256 , 1))\n    y = np.reshape(y , (256 ,256 , 1))\n    #y = y.astype(\"float32\")\n    #y_pred = y_pred.astype(\"float32\")\n    \n    \n    iou_score_list.append(iou_coeff(y , y_pred))\n    precision_score_list.append(precision(y , y_pred))\n    recall_score_list.append(recall(y , y_pred))\n    dice_coeff_score_list.append(dice_coeff(y , y_pred))\n\nprint(\"Average_IOU :\", np.mean(iou_score_list)) \nprint(\"Average_Precision :\", np.mean(precision_score_list)) \nprint(\"Average_Recall :\", np.mean(recall_score_list)) \nprint(\"Average_Dice_Coeff :\", np.mean(dice_coeff_score_list)) \n\nimport shutil\n\nshutil.make_archive(\"RESULT_Test\", 'zip', \"./RESULT_Test\")\n","metadata":{"id":"9zuUHlYwjXrs","execution":{"iopub.status.busy":"2021-09-23T18:20:37.312524Z","iopub.execute_input":"2021-09-23T18:20:37.312787Z","iopub.status.idle":"2021-09-23T18:22:42.222052Z","shell.execute_reply.started":"2021-09-23T18:20:37.312754Z","shell.execute_reply":"2021-09-23T18:22:42.221355Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\n\nFileLink(r'./RESULT_Test.zip')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T18:22:42.223328Z","iopub.execute_input":"2021-09-23T18:22:42.223765Z","iopub.status.idle":"2021-09-23T18:22:42.230820Z","shell.execute_reply.started":"2021-09-23T18:22:42.223729Z","shell.execute_reply":"2021-09-23T18:22:42.229981Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"patch_size=256\nfrom PIL import Image\nlarge_image = io.imread('../input/imagestest/Images_to_Test/1/images/i1_.png' , as_gray = False )\nlarge_image = large_image/ 255.0\nlarge_image = trans.resize(large_image, (2560, 2560), mode='constant')\n\npatches = patchify(large_image, ( 256 , 256 , 3), step= 256 )  #Step=256 for 256 patches means no overlap\n\npredicted_patches = []\nfor i in range(patches.shape[0]):\n    for j in range(patches.shape[1]):\n        #print(i,j)\n        \n        single_patch = patches[i,j,:,: ,:]   # (1 , 256 , 256 , 3)\n        \n        \n        #single_patch = single_patch/255.0\n        #single_patch_norm = np.expand_dims(np.array(single_patch), axis=1)\n        single_patch=np.expand_dims(single_patch, 0)\n        #print(single_patch.shape)\n        \n#Predict and threshold for values above 0.5 probability\n        single_patch_prediction = (model.predict(single_patch[0,:,:,:])[0] > 0.5)\n        single_patch_prediction = single_patch_prediction * 255.0\n        predicted_patches.append(single_patch_prediction)\n\npredicted_patches = np.array(predicted_patches)\n\n\npredicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 256,256) )\nreconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape[:2])\nplt.imshow(reconstructed_image, cmap='gray')\nplt.imsave('./segm1.png', reconstructed_image, cmap='gray')\n\nplt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n\nfinal_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(221)\nplt.title('Large Image')\nplt.imshow(large_image, cmap='gray')\nplt.subplot(222)\nplt.title('Prediction of large Image')\nplt.imshow(reconstructed_image, cmap='gray')\nplt.subplot(223)\nplt.title('final_prediction')\nplt.imshow(final_prediction, cmap='gray')\nplt.show()","metadata":{"id":"JcROpUrFjXrs","outputId":"e2364a58-083a-4e93-8e00-6690beee2c1c","execution":{"iopub.status.busy":"2021-09-23T18:22:42.232013Z","iopub.execute_input":"2021-09-23T18:22:42.232321Z","iopub.status.idle":"2021-09-23T18:22:53.733177Z","shell.execute_reply.started":"2021-09-23T18:22:42.232238Z","shell.execute_reply":"2021-09-23T18:22:53.732398Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"bYd43FdDjXrt","outputId":"2771ab44-fe22-40fb-baff-4b674f4b1312","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"hjfMb9BnjXrt","outputId":"69b96855-6e0e-42f1-f143-e37f10197229","trusted":true},"execution_count":null,"outputs":[]}]}